---
title: "Support Vector Machine"
date: 2023-04-25
permalink: /posts/2023/04/blog-post-2/
tags:
  - Machine Learning
  - Introduction
  - ML
---

The blog post introduces Support Vector Machine (SVM). It highlights the algorithm's compatibility with Google Colaboratory and presents two types of SVM - linear kernel and kernel trick. The blog also provides examples of classification and regression using SVM in Python. The post is compatible with Google Colaboratory and can be accessed through this link:

<a href="https://colab.research.google.com/github/arminnorouzi/machine_learning_course_UofA_MECE610/blob/main/L01_Introduction_to_Machine_Learning/L01b_Support_vector_Machine.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# L01b- Support Vector Machine

- Developed by **Armin Norouzi**
- Compatible with Google Colaboratory

**Table of Contents:**

1.  Support Vector Machine - linear kernel
2.  Support Vector Machine - kernel trick
3.  Examples in Python - classification
4.  Example in Python - Regression

### Support Vector Machine (SVM)- linear kernel

- is one of the supervised machine learning methods
- was introduced for a classification problem in 1964
- SVM is typically used for labeled data classification and function approximation (regression)
- by generating a set of hyperplanes in an infinite-dimensional space

- the application of SVM in the regression or so-called Support Vector Regression (SVR)
- was introduced by Vapkin in 1995.
- SVR is used for function approximation to find a correlation between labeled data.

- For a given learning data, $ \{\mathbf{u_i}, \mathbf{z_i} \}$,
  - where $\mathbf{u_i}$ is feature set and $\mathbf{z_i}$ is target output

**SVR -approximate function**

The approximate function for a given data set can be defined as

\begin{equation}
\mathbf{y(u_i)} = \mathbf{w}^T\mathbf{u_i} + \mathbf{b}
\end{equation}

- where $\mathbf{w}$ and $\mathbf{b}$ are found by solving the SVR algorithm

**SVR - solve a convex optimization**

- Thus, a convex optimization problem is defined as

\begin{equation}
\begin{split}
\textbf{Minimize: } \ & \frac{1}{2} ||\mathbf{w}||^2_2\\
\textbf{Subject to: } &
\begin{cases}
\ \mathbf{z_i} - \mathbf{w}^T\mathbf{u_i} - \mathbf{b} \leq \mathbf{\epsilon}\\
\ \mathbf{w}^T\mathbf{u_i} + \mathbf{b} - \mathbf{z_i} \leq \ -\mathbf{\epsilon}\\
\end{cases} \ \ \ \ i = 1, ..., n
\end{split}
\end{equation}

- If such a function, $\mathbf{y(u_i)}$

- which approximates all pairs of labeled learning data within a defined $\epsilon$ margin ($-\mathbf{\epsilon} \leq \mathbf{z_i}-\mathbf{y_i}  \leq \mathbf{\epsilon}$)
- and is as smooth as possible,

- is found by solving above optimization
  - the convex optimization problem is feasible.
  - Otherwise, the convex optimization is infeasible.

**Slack variables**

- Slack variables ($\mathbf{\zeta}^-\_i, \mathbf{\zeta}^+\_i $ ) are added as a penalty variable
- which regulates error tolerate of optimization as

\begin{equation}
-\mathbf{\epsilon} - \mathbf{\zeta}^-\_i \leq \mathbf{z_i} -\mathbf{y_i} \leq \mathbf{\epsilon} + \mathbf{\zeta}^+\_i
\end{equation}

**To overcome convex optimization problem in-feasibility**

- the Soft Margin Loss Function (SMLF) with definition of slack variables ( $\mathbf{\zeta}^-_i, \mathbf{\zeta}^+_i$) is used as

\begin{equation}
L\_{\epsilon} (\mathbf{z_i} , \mathbf{y_i}) =
\begin{cases}
0 & |\mathbf{z_i} - \mathbf{y_i}| \leq \mathbf{\epsilon} \\
|\mathbf{z_i} - \mathbf{y_i}| - \mathbf{\epsilon} & \text{otherwise} \\
\end{cases}
\end{equation}

- The SMLF is equal to summation of slack variables as

\begin{equation}
L\_{\epsilon} (\mathbf{\zeta}^-\_i , \mathbf{\zeta}^+\_i) = \mathbf{\zeta}^-\_i + \mathbf{\zeta}^+\_i
\end{equation}

**Soft Margin Loss Function (SMLF)**

The SMLF with slacks variables is shown schematically here

![](https://github.com/arminnorouzi/machine_learning_course_UofA_MECE610/blob/main/L03_Support_vector_Machine/figures/svm_soft_margin.PNG?raw=true)

SMLF of approximate function

- the convex problem is now defined to
  - optimize smoothness of approximate function and minimize the SMLF
  - and associated constraint as

$$
\begin{aligned}
&\textbf{Minimize: } && \frac{1}{2} ||\mathbf{w}||^2*2 + C \sum_{i = 1}^n (\mathbf{\zeta_i^+} + \mathbf{\zeta_i^-})\\
&\textbf{Subject to: } &&
\begin{cases}
\ \mathbf{z_i} - \mathbf{w}^T\mathbf{u_i} - \mathbf{b} \leq \mathbf{\epsilon} + \mathbf{\zeta_i^+}\\
\ \mathbf{w}^T\mathbf{u_i} + \mathbf{b} - \mathbf{z_i} \leq \mathbf{\epsilon} + \mathbf{\zeta_i^-}\\
\mathbf{\zeta_i^-}, \mathbf{\zeta_i^+} \geq 0 \\
\end{cases} \quad i = 1, ..., n
\end{aligned}
$$

where $C$ is a positive parameter which is used to adjust trade-off between model tolerated error and model smoothness.

**To solve the convex problem**

- the Lagrangian function (primal objective function) is calculated as

\begin{equation}
\begin{split}
L =& \frac{1}{2} ||\mathbf{w}||^2*2 + C \sum*{i = 1}^N \big(\mathbf{\zeta}^-_i + \mathbf{\zeta}^+\_i\big)\\
&- \sum_{i = 1}^N \alpha^+_i (-\mathbf{z_i} + \mathbf{y_i} + \mathbf{\epsilon} + \mathbf{\zeta}^+\_i) - \sum_{i = 1}^N \mu^+_i \mathbf{\zeta}^+\_i \\
&- \sum_{i = 1}^N \alpha^-_i (\mathbf{z_i} - \mathbf{y_i} + \mathbf{\epsilon} + \mathbf{\zeta}^-\_i) - \sum_{i = 1}^N \mu^-\_i \mathbf{\zeta}^-\_i
\end{split}
\end{equation}

where $\alpha^+_i$, $\alpha^-_i$, $\mu^+_i$, and $\mu^-_i$ are Lagrangian multipliers.

**Solve the optimization**

- by calculating the partial differential of Lagrangian function with respect to the primal variables equal as \cite{smola2004tutorial}

\begin{equation}
\begin{split}
\frac{\partial L}{\partial \mathbf{w}} = 0& \; \rightarrow \; \mathbf{w} = \sum*{i = 1}^N (\alpha_i^+ - \alpha_i^-)\mathbf{u_i} \\
\frac{\partial L}{\partial \mathbf{b}} = 0&~\rightarrow~ \sum*{i = 1}^N (\alpha_i^+ - \alpha_i^-) = 0 \\
\frac{\partial L}{\partial \mathbf{\zeta}^+\_i} = 0& \; \rightarrow \; \alpha_i^+ + \mu^+\_i = C \\
\frac{\partial L}{\partial \mathbf{\zeta}^-\_i} = 0& \; \rightarrow\; \alpha_i^- + \mu^-\_i = C
\end{split}
\end{equation}

**Dual optimization problem**

- Substituting these to optimization problem generates the dual optimization problem as:

\begin{equation}
\begin{split}
\textbf{Minimize:} \; L =& \frac{1}{2} \sum*{i = 1}^N \sum*{j = 1}^N (\alpha*i^+ - \alpha_i^-)(\alpha_j^+ - \alpha_j^-) \mathbf{u_i}^T\mathbf{u_j} \\
& + \sum*{i = 1}^N (\alpha*i^ + - \alpha_i^-)\mathbf{z_i} - \mathbf{\epsilon} \sum*{i = 1}^N (\alpha*i^+ + \alpha_i^-)\\
\textbf{Subject to:} \; &
\begin{cases}
& \sum*{i = 1}^N (\alpha_i^+ - \alpha_i^-) = 0\\
& 0 \leq \alpha_i^+ \leq C \\
& 0 \leq \alpha_i^- \leq C
\end{cases}
\end{split}
\end{equation}

**A standard Quadratic programming form (QP) is obtained**

- by rewriting prevous equation

\begin{equation}
\begin{split}
\textbf{Minimize:} \quad & \frac{1}{2} \alpha^T \mathbf{\mathcal{H}} \alpha + f^T \alpha \\
\textbf{Subject to:} \quad & A*{eq} \alpha = B*{eq}
\end{split}
\end{equation}

**Karush-Kuhn-Tucker (KKT) conditions**

- Now, $\alpha$ is found by solving QP equation
- and then substituting $\alpha$ into optimizaiton problem, $\mathbf{w}$ is obtained.
- To find $\mathbf{b}$, Karush-Kuhn-Tucker (KKT) conditions is applied to dual optimization problem
- Based on KKT conditions, the following equations must be fulfilled at the optimum point

\begin{equation}
\begin{split}
\alpha_i^+ (-\mathbf{z_i} + \mathbf{y_i} + \mathbf{\epsilon} + \mathbf{\zeta}\_i^+) = 0\\
\alpha_i^- (\mathbf{z_i} - \mathbf{y_i} + \mathbf{\epsilon} + \mathbf{\zeta}\_i^-) = 0 \\
\mu_i^+ \mathbf{\zeta}\_i^+ = (C- \alpha_i^+)\mathbf{\zeta}\_i^+\\
\mu_i^- \mathbf{\zeta}\_i^- = (C- \alpha_i^-)\mathbf{\zeta}\_i^-
\end{split}
\end{equation}

Based on our optimization problem constraints:

\begin{equation}
\begin{split}
\alpha_i^+ = \alpha_i^- = 0\\
0 < \alpha_i^+ < C, \ \alpha_i^- = 0 \\
0 < \alpha_i^- < C, \ \alpha_i^+ = 0\\
\alpha_i^+ = C, \ \alpha_i^- = 0\\
\alpha_i^- = C, \ \alpha_i^+ = 0
\end{split}
\end{equation}

**KKT - support vectors index**

- the exact value of $|\mathbf{z_i} - \mathbf{y_i}| = \mathbf{\epsilon}$ will be obtained
- and the set of data which the error between prediction and actual equals to $\mathbf{\epsilon}$, can be calculated as

\begin{equation}
S = \{\;i\;|\;0 < \alpha_i^+ \;+\; \alpha_i^- <C \}
\end{equation}

where $S$ is called support vectors index.

**Support vector set**

As the support vector set $ \mathbf{z_i} = \mathbf{y_i} + sign(\alpha_i^+ - \alpha_i^-)\mathbf{\epsilon}$ is true, $\mathbf{b}$ can be calculated as

\begin{equation}\label{eq:b}
\mathbf{b} = \frac{1}{|S|} \sum\_{i \in S}^S (\mathbf{z_i} -\mathbf{w}^T\mathbf{u_i} - sign(\alpha_i^+ - \alpha_i^-)\mathbf{\epsilon})
\end{equation}

## Support Vector Machine - kernel trick

- To improve the performance of conventional SVM in providing distribution predictions
- an implicit feature space is used instead of simply computing the inner product in prevous section,
- $\mathbf{u_i}^T\mathbf{u_j}$ (a high-dimensional map).
- In this case the optimization problem is to find the smoothest function in the feature space instead of the input space
- The nonlinear kernel is now used by mapping the training pattern into the feature space instead of using the term $\mathbf{u_i}^T \mathbf{u_j}$.

- for non-linear SVM, a low-dimensional input space with a high-dimensional feature space is used,
- while for a linear SVM a high-dimensional linear input space is used.

- Thus, the kernel is defined as $K(\mathbf{u_i},\mathbf{u_j})$, and used instead of $\mathbf{u_i}^T\mathbf{u_j}$.
- Therefore, ${H}$ is changed with ${H} = \left[ K(\mathbf{u_i},\mathbf{u_j})  \right]$.

**Kernel based dual problem**

- So, the dual problem for kernel-based SVM is obtained as

\begin{equation}
\begin{split}
\textbf{Minimize:} \; L =& \frac{1}{2} \sum*{i = 1}^N \sum*{j = 1}^N (\alpha*i^+ - \alpha_i^-)(\alpha_j^+ - \alpha_j^-)K(\mathbf{u_i},\mathbf{u_j}) \\
& + \sum*{i = 1}^N (\alpha*i^+ - \alpha_i^-)\mathbf{z_i} - \mathbf{\epsilon} \sum*{i = 1}^N (\alpha*i^+ + \alpha_i^-)\\
\textbf{Subject to:} \; &
\begin{cases}
& \sum*{i = 1}^N (\alpha_i^+ - \alpha_i^-) = 0\\
& 0 \leq \alpha_i^+ \leq C \\
& 0 \leq \alpha_i^- \leq C
\end{cases}
\end{split}
\end{equation}

**Kernel Based - QP problem**

- $\mathbf{w}$ is calculated as $w = \sum_{i = 1}^N (\alpha_i^+ - \alpha_i^-)\mathbf{u_i}$,
- which is linear combination of training data
- and using kernel function instead of inner product ($K(\mathbf{u_i},\mathbf{u_j})$ instead of $\mathbf{u_i}\mathbf{u}$),
- the approximate function is calculated as

\begin{equation}
\mathbf{y} = \sum\_{i=1}^n (\alpha_i^+ - \alpha_i^-)K(\mathbf{u_i},\mathbf{u}) + \mathbf{b}
\end{equation}

**Special case, radial basis function (RBF) kernel is used in kernel based SVM**

- then it can be represent as Radial Basis Function Network (RBFN) shown schematically

![](https://github.com/arminnorouzi/machine_learning_course_UofA_MECE610/blob/main/L03_Support_vector_Machine/figures/kernel_svm.PNG?raw=true)

- The RBF kernel function is defined as

\begin{equation}
K(\mathbf{u_i},\mathbf{u_j}) = exp \bigg( \frac{||\mathbf{u_i} - \mathbf{u_j}||\_2^2}{2 \sigma^2} \bigg)
\end{equation}

- where $\sigma$ is free variables that acts like
  - Gaussian variance in probability density function of a normally distributed random variable
- and $||.||_2$ is the Euclidean norm.

## SVM Example classification - Python toolbox

**Using toolbox of SVM- classification**

```python
#import necessary libraries
import pandas as pd
import numpy as np
import math
import operator
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
```

```python
import sklearn
import sklearn.datasets
import sklearn.linear_model
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
```

**Import data and show**

```python
data = pd.read_csv('https://raw.githubusercontent.com/arminnorouzi/ML-developed_course/main/Data/Engine_NOx_classification.csv')
data.head()
```

  <div id="df-a5aa33ce-b70f-45f9-9db6-a998b0ba8945">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Load [ft.lb]</th>
      <th>Engine speed [rpm]</th>
      <th>mf [mg/stroke]</th>
      <th>Pr [PSI]</th>
      <th>NOx [ppm]</th>
      <th>High NOx</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>50</td>
      <td>2502.400000</td>
      <td>31.222326</td>
      <td>15285.16744</td>
      <td>103.899724</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>50</td>
      <td>2248.666667</td>
      <td>30.116667</td>
      <td>15155.13333</td>
      <td>112.610181</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>75</td>
      <td>2502.000000</td>
      <td>38.300000</td>
      <td>15356.00000</td>
      <td>114.789893</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>100</td>
      <td>2504.000000</td>
      <td>42.900000</td>
      <td>15296.00000</td>
      <td>125.411970</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>75</td>
      <td>2262.000000</td>
      <td>34.100000</td>
      <td>15254.00000</td>
      <td>126.524679</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-a5aa33ce-b70f-45f9-9db6-a998b0ba8945')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
width="24px">
<path d="M0 0h24v24H0V0z" fill="none"/>
<path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
</svg>
</button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-a5aa33ce-b70f-45f9-9db6-a998b0ba8945 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-a5aa33ce-b70f-45f9-9db6-a998b0ba8945');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>

  </div>

**Setup test and training data and plot**

```python
cdf = data[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'High NOx']]

msk = np.random.rand(len(data)) < 0.8
train = cdf[msk]
test = cdf[~msk]
```

```python
colors = {0: 'blue', 1:'red', 2:'green', 3:'coral', 4:'orange', 5:'black'}

area = 150
# area = 200
plt.figure(1, figsize=(10, 4))
plt.scatter(train['Load [ft.lb]'], train['Engine speed [rpm]'], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)

plt.xlabel('Load [ft.lb]', fontsize=12)
plt.ylabel('Engine speed [rpm]', fontsize=12)
```

    Text(0, 0.5, 'Engine speed [rpm]')

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L01b_Support_vector_Machine_files/L01b_Support_vector_Machine_50_1.png)

**Plot test set**

```python
area = 150
# area = 200
plt.figure(1, figsize=(10, 4))
plt.scatter(test['Load [ft.lb]'], test['Engine speed [rpm]'], s=area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)

plt.xlabel('Load [ft.lb]', fontsize=12)
plt.ylabel('Engine speed [rpm]', fontsize=12)
```

    Text(0, 0.5, 'Engine speed [rpm]')

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L01b_Support_vector_Machine_files/L01b_Support_vector_Machine_52_1.png)

**Pre- processing**

```python
from sklearn import preprocessing


train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]']])
train_y = np.asanyarray(train[['High NOx']])

test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]']])
test_y = np.asanyarray(test[['High NOx']])


min_max_scaler = preprocessing.MinMaxScaler()
X_train_minmax = min_max_scaler.fit_transform(train_x)
X_test_minmax = min_max_scaler.transform(test_x)
```

**Linear SVM classifier**

```python
X = X_train_minmax
Y = train_y

Xts = X_test_minmax
Yts = test_y

from sklearn import svm
```

```python
clf = svm.SVC(kernel='linear')
clf.fit(X, np.ravel(Y,order='C'))
```

    SVC(kernel='linear')

```python
clf.predict(X)
```

    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1])

**Linear SVM - now plot**

```python
colors = {0: 'lightcoral', 1:'aqua', 2:'green', 3:'coral', 4:'orange', 5:'black'}

X_plot = X_train_minmax.T
Y_plot = train_y.T

X_plot_ts = X_test_minmax.T
Y_plot_ts = test_y.T

# Set min and max values and give it some padding
x_min, x_max = X_plot[0, :].min() - 0.1, X_plot[0, :].max() + 0.1
y_min, y_max = X_plot[1, :].min() - 0.1, X_plot[1, :].max() + 0.1
h = 0.01
# Generate a grid of points with distance h between them
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

Z = clf.predict(np.c_[xx.ravel().T, yy.ravel().T])
Z = Z.reshape(xx.shape)
# Plot the contour and training examples
plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
area = 30
plt.scatter(X_plot[0,:], X_plot[1,:], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)
plt.scatter(X_plot_ts[0,:], X_plot_ts[1,:], s=8*area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)
plt.xlabel('Load [ft.lb]', fontsize=18)
plt.ylabel('Engine speed [rpm]', fontsize=16)


predictions = clf.predict(X)
predictions_test = clf.predict(Xts)
accuracy = float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.size)*100)
accuracy_test = float((np.dot(Yts.T,predictions_test.T) + np.dot(1-Yts.T,1-predictions_test.T))/float(Yts.size)*100)
print ("Traing accuracy: {} %".format(accuracy))
print ("Testing accuracy: {} %".format(accuracy_test))
```

    Traing accuracy: 93.61702127659575 %
    Testing accuracy: 93.33333333333333 %

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L01b_Support_vector_Machine_files/L01b_Support_vector_Machine_60_1.png)

**RBF SVM classifier**

```python
clf_n = svm.SVC(kernel='rbf')
clf_n.fit(X, np.ravel(Y,order='C'))
```

    SVC()

**Plot RBF SVM Classifier**

```python
colors = {0: 'lightcoral', 1:'aqua', 2:'green', 3:'coral', 4:'orange', 5:'black'}

X_plot = X_train_minmax.T
Y_plot = train_y.T

X_plot_ts = X_test_minmax.T
Y_plot_ts = test_y.T

# Set min and max values and give it some padding
x_min, x_max = X_plot[0, :].min() - 0.1, X_plot[0, :].max() + 0.1
y_min, y_max = X_plot[1, :].min() - 0.1, X_plot[1, :].max() + 0.1
h = 0.01
# Generate a grid of points with distance h between them
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

Z = clf_n.predict(np.c_[xx.ravel().T, yy.ravel().T])
Z = Z.reshape(xx.shape)
# Plot the contour and training examples
plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
area = 30
plt.scatter(X_plot[0,:], X_plot[1,:], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)
plt.scatter(X_plot_ts[0,:], X_plot_ts[1,:], s=8*area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)
plt.xlabel('Load [ft.lb]', fontsize=18)
plt.ylabel('Engine speed [rpm]', fontsize=16)

predictions = clf_n.predict(X)
predictions_test = clf_n.predict(Xts)
accuracy = float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.size)*100)
accuracy_test = float((np.dot(Yts.T,predictions_test.T) + np.dot(1-Yts.T,1-predictions_test.T))/float(Yts.size)*100)
print ("Traing accuracy: {} %".format(accuracy))
print ("Testing accuracy: {} %".format(accuracy_test))
```

    Traing accuracy: 97.87234042553192 %
    Testing accuracy: 86.66666666666667 %

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L01b_Support_vector_Machine_files/L01b_Support_vector_Machine_64_1.png)

**Polynomial SVM classifier**

```python
clf_n = svm.SVC(kernel='poly', degree = 2)
clf_n.fit(X, np.ravel(Y,order='C'))
```

    SVC(degree=2, kernel='poly')

**Plot Polynomial SVM Classifier**

```python
colors = {0: 'lightcoral', 1:'aqua', 2:'green', 3:'coral', 4:'orange', 5:'black'}

X_plot = X_train_minmax.T
Y_plot = train_y.T

X_plot_ts = X_test_minmax.T
Y_plot_ts = test_y.T

# Set min and max values and give it some padding
x_min, x_max = X_plot[0, :].min() - 0.1, X_plot[0, :].max() + 0.1
y_min, y_max = X_plot[1, :].min() - 0.1, X_plot[1, :].max() + 0.1
h = 0.01
# Generate a grid of points with distance h between them
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

Z = clf_n.predict(np.c_[xx.ravel().T, yy.ravel().T])
Z = Z.reshape(xx.shape)
# Plot the contour and training examples
plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
area = 30
plt.scatter(X_plot[0,:], X_plot[1,:], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)
plt.scatter(X_plot_ts[0,:], X_plot_ts[1,:], s=8*area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)
plt.xlabel('Load [ft.lb]', fontsize=18)
plt.ylabel('Engine speed [rpm]', fontsize=16)


predictions = clf_n.predict(X)
predictions_test = clf_n.predict(Xts)
accuracy = float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.size)*100)
accuracy_test = float((np.dot(Yts.T,predictions_test.T) + np.dot(1-Yts.T,1-predictions_test.T))/float(Yts.size)*100)
print ("Traing accuracy: {} %".format(accuracy))
print ("Testing accuracy: {} %".format(accuracy_test))
```

    Traing accuracy: 91.48936170212765 %
    Testing accuracy: 93.33333333333333 %

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L01b_Support_vector_Machine_files/L01b_Support_vector_Machine_68_1.png)

**Effect of Regularization parameter (C)**

```python
plt.figure(figsize=(16, 32))
CC = [0.1, 1, 10, 100, 1000]
for i, c in enumerate(CC):

    plt.subplot(5, 2, i+1)
    plt.title('Regularization %1.6f' %c)
    clf = svm.SVC(kernel='rbf', C=c)
    clf.fit(X, np.ravel(Y,order='C'))


    predictions = clf.predict(X)
    predictions_test = clf.predict(Xts)
    accuracy = float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.size)*100)
    accuracy_test = float((np.dot(Yts.T,predictions_test.T) + np.dot(1-Yts.T,1-predictions_test.T))/float(Yts.size)*100)
    print ("Accuracy for C = {}: {} %".format(c, accuracy))
    print ("Accuracy test for C = {}: {} %".format(c, accuracy_test))

    X_plot = X_train_minmax.T
    Y_plot = train_y.T

    X_plot_ts = X_test_minmax.T
    Y_plot_ts = test_y.T

    # Set min and max values and give it some padding
    x_min, x_max = X_plot[0, :].min() - 0.1, X_plot[0, :].max() + 0.1
    y_min, y_max = X_plot[1, :].min() - 0.1, X_plot[1, :].max() + 0.1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    Z = clf.predict(np.c_[xx.ravel().T, yy.ravel().T])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    area = 30
    plt.scatter(X_plot[0,:], X_plot[1,:], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)
    plt.scatter(X_plot_ts[0,:], X_plot_ts[1,:], s=8*area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)
    plt.xlabel('Load [ft.lb]', fontsize=18)
    plt.ylabel('Engine speed [rpm]', fontsize=16)
```

    Accuracy for C = 0.1: 89.36170212765957 %
    Accuracy test for C = 0.1: 93.33333333333333 %
    Accuracy for C = 1: 97.87234042553192 %
    Accuracy test for C = 1: 86.66666666666667 %
    Accuracy for C = 10: 97.87234042553192 %
    Accuracy test for C = 10: 86.66666666666667 %
    Accuracy for C = 100: 97.87234042553192 %
    Accuracy test for C = 100: 86.66666666666667 %
    Accuracy for C = 1000: 97.87234042553192 %
    Accuracy test for C = 1000: 86.66666666666667 %

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L01b_Support_vector_Machine_files/L01b_Support_vector_Machine_70_1.png)

## SVM Example Regression - use Toolbox

**Using toolbox of SVM- Regression**

- import the necessary libraries

```python
import pandas as pd
import numpy as np
import math
import operator
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')


import sklearn
import sklearn.datasets
import sklearn.linear_model
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
```

**Import the data and view and plot**

```python
data = pd.read_csv('https://raw.githubusercontent.com/arminnorouzi/ML-developed_course/main/Data/Engine_NOx_classification.csv')
data.head()
```

  <div id="df-5cfbe1c7-8c03-4367-9327-29b088dd1c78">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Load [ft.lb]</th>
      <th>Engine speed [rpm]</th>
      <th>mf [mg/stroke]</th>
      <th>Pr [PSI]</th>
      <th>NOx [ppm]</th>
      <th>High NOx</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>50</td>
      <td>2502.400000</td>
      <td>31.222326</td>
      <td>15285.16744</td>
      <td>103.899724</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>50</td>
      <td>2248.666667</td>
      <td>30.116667</td>
      <td>15155.13333</td>
      <td>112.610181</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>75</td>
      <td>2502.000000</td>
      <td>38.300000</td>
      <td>15356.00000</td>
      <td>114.789893</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>100</td>
      <td>2504.000000</td>
      <td>42.900000</td>
      <td>15296.00000</td>
      <td>125.411970</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>75</td>
      <td>2262.000000</td>
      <td>34.100000</td>
      <td>15254.00000</td>
      <td>126.524679</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-5cfbe1c7-8c03-4367-9327-29b088dd1c78')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
width="24px">
<path d="M0 0h24v24H0V0z" fill="none"/>
<path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
</svg>
</button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-5cfbe1c7-8c03-4367-9327-29b088dd1c78 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-5cfbe1c7-8c03-4367-9327-29b088dd1c78');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>

  </div>

```python
cdf = data[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'NOx [ppm]']]

msk = np.random.rand(len(data)) < 0.8
train = cdf[msk]
test = cdf[~msk]
```

```python
font = {'family' : 'normal',
        'weight' : 'normal',
        'size'   : 12}

plt.rc('font', **font)

plt.figure(1, figsize=(10, 4))
plt.plot(train['Load [ft.lb]'], train['NOx [ppm]'], 'or', label = 'train')
plt.plot(test['Load [ft.lb]'], test['NOx [ppm]'], 'ob', label = 'test')
plt.xlabel('Load [ft.lb]', fontsize=12)
plt.ylabel('NOx [ppm]', fontsize=12)
plt.legend(fontsize=12)
```

    <matplotlib.legend.Legend at 0x7f9138791b90>



    WARNING:matplotlib.font_manager:findfont: Font family ['normal'] not found. Falling back to DejaVu Sans.

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L01b_Support_vector_Machine_files/L01b_Support_vector_Machine_78_2.png)

**Setup Training and test data**

```python
from sklearn import preprocessing

train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])


test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])



train_y = np.asanyarray(train[['NOx [ppm]']])


test_y = np.asanyarray(test[['NOx [ppm]']])


min_max_scaler = preprocessing.MinMaxScaler()
X_train_minmax = min_max_scaler.fit_transform(train_x)
X_test_minmax = min_max_scaler.transform(test_x)
```

**Use a Linear SVM regressor**

- set training and test data

```python
X = X_train_minmax
Y = train_y

Xts = X_test_minmax
Yts = test_y

from sklearn import svm
```

**Set Linear SVR kernel fit and plot**

```python
clf = svm.SVR(kernel='linear', C=100.0, epsilon=0.1)
clf.fit(X, train_y.ravel())

train_y_hat = clf.predict(X)
test_y_hat = clf.predict(Xts)
```

```python
plt.figure(1, figsize=(6, 6))
plt.plot([100,350], [100, 350],  '-k')
plt.plot(train_y_hat, train['NOx [ppm]'], 'ob', label = 'train')
plt.plot(test_y_hat, test['NOx [ppm]'], 'or', label = 'test')
plt.ylabel("Exp NOx [ppm]", fontsize=18)
plt.xlabel("Pred NOx [ppm]", fontsize=18)
plt.legend(fontsize=18)
```

    <matplotlib.legend.Legend at 0x7f91386ff750>



    WARNING:matplotlib.font_manager:findfont: Font family ['normal'] not found. Falling back to DejaVu Sans.

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L01b_Support_vector_Machine_files/L01b_Support_vector_Machine_86_2.png)

**Linear SVM find the R-squared**

```python
from sklearn.metrics import r2_score

r2_test = r2_score(test_y_hat, test_y.ravel())
r2_train = r2_score(train_y_hat, train_y.ravel())

print ('Train R^2= %1.3f' % r2_train)
print ('Test R^2= %1.3f' % r2_test)
```

    Train R^2= 0.861
    Test R^2= 0.834

**Now try RBF SVM regressor - plot and R-squared**

```python
clf = svm.SVR(kernel='rbf', C=100, epsilon=1)
clf.fit(X, train_y.ravel())

train_y_hat = clf.predict(X)
test_y_hat = clf.predict(Xts)


plt.figure(1, figsize=(6, 6))
plt.plot([100,350], [100, 350],  '-k')
plt.plot(train_y_hat, train['NOx [ppm]'], 'ob', label = 'train')
plt.plot(test_y_hat, test['NOx [ppm]'], 'or', label = 'test')
plt.ylabel("Exp NOx [ppm]", fontsize=12)
plt.xlabel("Pred NOx [ppm]", fontsize=12)
plt.legend(fontsize=12)


r2_test = clf.score(X_test_minmax, test_y.ravel())
r2_train = clf.score(X_train_minmax, train_y.ravel())

print ('Train R^2= %1.3f' % r2_train)
print ('Test R^2= %1.3f' % r2_test)
```

    Train R^2= 0.985
    Test R^2= 0.921

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L01b_Support_vector_Machine_files/L01b_Support_vector_Machine_90_1.png)

**Look at the effect of Regularization parameter (C) with RBF kernel**

```python
plt.figure(figsize=(16, 42))
CC = [0.1, 1, 10, 100, 1000]
for i, c in enumerate(CC):

    plt.subplot(5, 2, i+1)
    plt.title('Regularization %1.6f' %c)
    clf = svm.SVR(kernel='rbf', C=c)
    clf.fit(X, train_y.ravel())


    train_y_hat = clf.predict(X)
    test_y_hat = clf.predict(Xts)
    r2_test = clf.score(X_test_minmax, test_y.ravel())
    r2_train = clf.score(X_train_minmax, train_y.ravel())

    print ("Accuracy for C = {}: {} %".format(c, r2_train))
    print ("Accuracy test for C = {}: {} %".format(c, r2_test))


    plt.plot([100,350], [100, 350],  '-k')
    plt.plot(train_y_hat, train['NOx [ppm]'], 'ob', label = 'train')
    plt.plot(test_y_hat, test['NOx [ppm]'], 'or', label = 'test')
    plt.ylabel("Exp NOx [ppm]", fontsize=12)
    plt.xlabel("Pred NOx [ppm]", fontsize=12)
    plt.legend(fontsize=12)
```

    WARNING:matplotlib.font_manager:findfont: Font family ['normal'] not found. Falling back to DejaVu Sans.


    Accuracy for C = 0.1: 0.0137704771518673 %
    Accuracy test for C = 0.1: -0.8443829009607502 %
    Accuracy for C = 1: 0.1918716728005846 %
    Accuracy test for C = 1: -0.268604569871812 %
    Accuracy for C = 10: 0.8479210196683147 %
    Accuracy test for C = 10: 0.739343978662603 %
    Accuracy for C = 100: 0.9851982385072424 %
    Accuracy test for C = 100: 0.933503545990004 %
    Accuracy for C = 1000: 0.9917688057292464 %
    Accuracy test for C = 1000: 0.968320530327154 %

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L01b_Support_vector_Machine_files/L01b_Support_vector_Machine_92_2.png)

## References

[1] Géron, Aurélien. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems. " O'Reilly Media, Inc.", 2019.

[2] https://www.coursera.org/specializations/deep-learning?utm_source=gg&utm_medium=sem&utm_content=01-CatalogDSA-ML2-US&campaignid=12490862811&adgroupid=119269357576&device=c&keyword=&matchtype=&network=g&devicemodel=&adpostion=&creativeid=503940597764&hide_mobile_promo&gclid=CjwKCAjw8sCRBhA6EiwA6_IF4Rtt8Tj3h_VRvx2wJA2vdlkTnb2bPWMTqZ48fXQPN01UwUzHY70cnhoCt38QAvD_BwE
