---
title: "Natural Language Processing using TensorFlow"
date: 2023-05-09
permalink: /posts/2023/05/blog-post-4/
tags:
  - Natural Language Processing
  - Transfer learning
  - TensorFlow
---

This blog post is a comprehensive guide to Natural Language Processing (NLP) using TensorFlow and it is compatible with Google Colaboratory and TensorFlow 2.8.2. The objective is to provide readers with a better understanding of how to use pre-trained models and perform transfer learning for NLP applications. The post covers a range of topics related to NLP, starting with an introduction to the field and the basics of data preparation. The post also includes visualization using TensorBoard, ensemble models, and prediction on the test data set. This post is an excellent resource for anyone looking to learn about NLP and how to implement it using TensorFlow.

The post is compatible with Google Colaboratory and can be accessed through this link:

<a href="https://colab.research.google.com/github/arminnorouzi/machine_learning_course_UofA_MECE610/blob/main/L04_Deep_Learning_with_TensorFlow/L04d_Natural_Language_Processing_with_TensorFlow.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Natural Language Processing using TensorFlow

- Developed by **Armin Norouzi**
- Compatible with Google Colaboratory- Tensorflow 2.8.2

- **Objective:** Using pretrained model and perfrom different kinds of transfer learning

**Table of content:**

1. Introduction to Natural Language Processing (NLP)
2. Load and prepare data
3. Converting text into numbers
4. Modeling
5. Comparing the performance of each of our models
6. Visualize in Tensorboard
7. Ensemble models
8. Prediction on the test data set

## Introduction to Trasnfer Learning (NLP)

The main goal of [natural language processing (NLP)](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32) is to derive information from natural language.

Natural language is a broad term but you can consider it to cover any of the following:

- Text (such as that contained in an email, blog post, book, Tweet)
- Speech (a conversation you have with a doctor, voice commands you give to a smart speaker)

Under the umbrellas of text and speech there are many different things you might want to do.

### Helper Function

These functions developed in previous lectures

```python
# Create function to unzip a zipfile into current working directory
# (since we're going to be downloading and unzipping a few files)
import zipfile

def unzip_data(filename):
  """
  Unzips filename into the current working directory.
  Args:
    filename (str): a filepath to a target zip folder to be unzipped.
  """
  zip_ref = zipfile.ZipFile(filename, "r")
  zip_ref.extractall()
  zip_ref.close()
```

```python
# Plot the validation and training data separately
import matplotlib.pyplot as plt

def plot_loss_curves(history):
  """
  Returns separate loss curves for training and validation metrics.
  Args:
    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)
  """
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();
```

```python
# Create tensorboard callback (functionized because need to create a new one for each model)
import datetime
import tensorflow as tf

def create_tensorboard_callback(dir_name, experiment_name):
  ''' This function is used to create tensorboard callback

  Arg:
      dir_name: overall logs directory
      experiment_name: particular experiment
      current_timestamp: time the experiment started based on Python's datetime.datetime().now()
  '''
  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir
  )
  print(f"Saving TensorBoard log files to: {log_dir}")
  return tensorboard_callback
```

```python
def compare_historys(original_history, new_history, initial_epochs=5):
    """
    Compares two TensorFlow model History objects.

    Args:
      original_history: History object from original model (before new_history)
      new_history: History object from continued model training (after original_history)
      initial_epochs: Number of epochs in original_history (new_history plot starts from here)
    """

    # Get original history measurements
    acc = original_history.history["accuracy"]
    loss = original_history.history["loss"]

    val_acc = original_history.history["val_accuracy"]
    val_loss = original_history.history["val_loss"]

    # Combine original history with new history
    total_acc = acc + new_history.history["accuracy"]
    total_loss = loss + new_history.history["loss"]

    total_val_acc = val_acc + new_history.history["val_accuracy"]
    total_val_loss = val_loss + new_history.history["val_loss"]

    # Make plots
    plt.figure(figsize=(8, 8))
    plt.subplot(2, 1, 1)
    plt.plot(total_acc, label='Training Accuracy')
    plt.plot(total_val_acc, label='Validation Accuracy')
    plt.plot([initial_epochs-1, initial_epochs-1],
              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    plt.subplot(2, 1, 2)
    plt.plot(total_loss, label='Training Loss')
    plt.plot(total_val_loss, label='Validation Loss')
    plt.plot([initial_epochs-1, initial_epochs-1],
              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.xlabel('epoch')
    plt.show()

```

## Load and prepare data

### Download data

Let's start by download a text dataset. We'll be using the [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) datset from Kaggle which contains text-based Tweets about natural disasters. The original downloaded data has not been altered to how you would download it from Kaggle:

- `sample_submission.csv` - an example of the file you'd submit to the Kaggle competition of your model's predictions.
- `train.csv` - training samples of real and not real diaster Tweets.
- `test.csv` - testing samples of real and not real diaster Tweets.

```python
# Turn .csv files into pandas DataFrame's
import pandas as pd
train_df = pd.read_csv("https://raw.githubusercontent.com/arminnorouzi/machine_learning_course_UofA_MECE610/main/Data/NLP_train.csv")
train_df.head()
```

  <div id="df-54a003c6-1526-4ace-9e8e-e8963a2bcc54">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>keyword</th>
      <th>location</th>
      <th>text</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Our Deeds are the Reason of this #earthquake M...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Forest fire near La Ronge Sask. Canada</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>All residents asked to 'shelter in place' are ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>13,000 people receive #wildfires evacuation or...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Just got sent this photo from Ruby #Alaska as ...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-54a003c6-1526-4ace-9e8e-e8963a2bcc54')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
width="24px">
<path d="M0 0h24v24H0V0z" fill="none"/>
<path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
</svg>
</button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-54a003c6-1526-4ace-9e8e-e8963a2bcc54 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-54a003c6-1526-4ace-9e8e-e8963a2bcc54');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>

  </div>

```python
test_df = pd.read_csv("https://raw.githubusercontent.com/arminnorouzi/machine_learning_course_UofA_MECE610/main/Data/NLP_test.csv")
test_df.head()
```

  <div id="df-c0829771-6de1-449b-8e72-271702720929">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>keyword</th>
      <th>location</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Just happened a terrible car crash</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Heard about #earthquake is different cities, s...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>there is a forest fire at spot pond, geese are...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Apocalypse lighting. #Spokane #wildfires</td>
    </tr>
    <tr>
      <th>4</th>
      <td>11</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-c0829771-6de1-449b-8e72-271702720929')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
width="24px">
<path d="M0 0h24v24H0V0z" fill="none"/>
<path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
</svg>
</button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-c0829771-6de1-449b-8e72-271702720929 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-c0829771-6de1-449b-8e72-271702720929');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>

  </div>

Suffling dataset

```python
# Shuffle training dataframe
train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility
train_df_shuffled.head()
```

  <div id="df-6579f241-08b5-4ee0-9280-97fd82071cc6">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>keyword</th>
      <th>location</th>
      <th>text</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2644</th>
      <td>3796</td>
      <td>destruction</td>
      <td>NaN</td>
      <td>So you have a new weapon that can cause un-ima...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2227</th>
      <td>3185</td>
      <td>deluge</td>
      <td>NaN</td>
      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5448</th>
      <td>7769</td>
      <td>police</td>
      <td>UK</td>
      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>132</th>
      <td>191</td>
      <td>aftershock</td>
      <td>NaN</td>
      <td>Aftershock back to school kick off was great. ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6845</th>
      <td>9810</td>
      <td>trauma</td>
      <td>Montgomery County, MD</td>
      <td>in response to trauma Children of Addicts deve...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-6579f241-08b5-4ee0-9280-97fd82071cc6')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
width="24px">
<path d="M0 0h24v24H0V0z" fill="none"/>
<path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
</svg>
</button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-6579f241-08b5-4ee0-9280-97fd82071cc6 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-6579f241-08b5-4ee0-9280-97fd82071cc6');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>

  </div>

```python
# How many examples of each class?
train_df.target.value_counts()
```

    0    4342
    1    3271
    Name: target, dtype: int64

```python
print("Percentage of negetive class:", round(100*4342/(3271 + 4342),2))
print("Percentage of positive class:", round(100*3271/(3271 + 4342),2))
```

    Percentage of negetive class: 57.03
    Percentage of positive class: 42.97

Since we have two target values, we're dealing with a **binary classification** problem.

It's fairly balanced too, about 57% negative class (`target = 0`) and 43% positive class (`target = 1`).

Where,

- `1` = a real disaster Tweet
- `0` = not a real disaster Tweet

And what about the total number of samples we have?

```python
print("Percentage of training data:", round(100*len(train_df)/(len(train_df) + len(test_df)), 2))
print("Percentage of testing data:", round(100*len(test_df)/(len(train_df) + len(test_df)), 2))
```

    Percentage of training data: 70.0
    Percentage of testing data: 30.0

```python
# Let's visualize some random training examples
import random
random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples
for row in train_df_shuffled[["text", "target"]][random_index:random_index + 5].itertuples():
  _, text, target = row
  print(f"Target: {target}", "(real disaster)" if target > 0 else "(not real disaster)")
  print(f"Text:\n{text}\n")
  print("---\n")
```

    Target: 0 (not real disaster)
    Text:
    Emergency Services Committee and Personnel Committee Meeting on Thursday Night http://t.co/DrBcRyPj4p

    ---

    Target: 0 (not real disaster)
    Text:
    http://t.co/JwIv6WYW6F Osage Beach releases name

    ---

    Target: 0 (not real disaster)
    Text:
    Drunk Meals 101: What To Cook When You're Totally Obliterated http://t.co/QvS7O10bG3

    ---

    Target: 1 (real disaster)
    Text:
    Drunk Meals 101: What To Cook When You're Totally Obliterated http://t.co/m19iVWrdkk

    ---

    Target: 0 (not real disaster)
    Text:
    U.S National Park Services Tonto National Forest: Stop the Annihilation of the Salt River Wild Horse... https://t.co/x2Wn7O2a3w via @Change

    ---

### Split data into training and validation sets

```python
from sklearn.model_selection import train_test_split

# Use train_test_split to split training data into training and validation sets
train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled["text"].to_numpy(),
                                                                            train_df_shuffled["target"].to_numpy(),
                                                                            test_size=0.1, # dedicate 10% of samples to validation set
                                                                            random_state=42) # random state for reproducibility
```

```python
# How many samples total?
print(f"Total training samples: {len(train_sentences)}")
print(f"Total validation samples: {len(val_sentences)}")
```

    Total training samples: 6851
    Total validation samples: 762

## Converting text into numbers

In NLP, there are two main concepts for turning text into numbers:

- **Tokenization:** A straight mapping from word or character or sub-word to a numerical value.

  1. **Word-level tokenization:** In this case, every word in a sequence considered a single _token_. With the sentence "I love TensorFlow" might result in "I" being `0`, "love" being `1` and "TensorFlow" being `2`
  2. **Character-level tokenization:** In this case, every character in a sequence considered a single _token_.
  3. **Sub-word tokenization:** This method is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, "my favourite food is pineapple pizza" might become "my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple _tokens_.

- **Embeddings:** An embedding is a representation of natural language which can be learned. Representation comes in the form of a _feature vector_. For example, the word "dance" could be represented by the 5-dimensional vector `[-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]`. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings:
  1. **Create your own embedding:** Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)) and an embedding representation will be learned during model training.
  2. **Reuse a pre-learned embedding:** Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task. Good model to start:
  - [Word2vec embeddings](https://jalammar.github.io/illustrated-word2vec/)
  - [GloVe embeddings](https://nlp.stanford.edu/projects/glove/)

### Text vectorization (tokenization)

To tokenize our words, we'll use the helpful preprocessing layer [`tf.keras.layers.experimental.preprocessing.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).

The `TextVectorization` layer takes the following parameters:

- `max_tokens`: The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens.
- `standardize`: Method for standardizing text. Default is `"lower_and_strip_punctuation"` which lowers text and removes all punctuation marks.
- `split`: How to split text, default is `"whitespace"` which splits on spaces.
- `ngrams`: How many words to contain per token split, for example, `ngrams=2` splits tokens into continuous sequences of 2.
- `output_mode`: How to output tokens, can be `"int"` (integer mapping), `"binary"` (one-hot encoding), `"count"` or `"tf-idf"`. See documentation for more.
- `output_sequence_length`: Length of tokenized sequence to output. For example, if `output_sequence_length=150`, all tokenized sequences will be 150 tokens long.
- `pad_to_max_tokens`: Defaults to `False`, if `True`, the output feature axis will be padded to `max_tokens` even if the number of unique tokens in the vocabulary is less than `max_tokens`. Only valid in certain modes, see docs for more.

```python
import tensorflow as tf
from tensorflow.keras.layers import TextVectorization

# Use the default TextVectorization variables
text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)
                                    standardize="lower_and_strip_punctuation", # how to process text
                                    split="whitespace", # how to split tokens
                                    ngrams=None, # create groups of n-words?
                                    output_mode="int", # how to map tokens to numbers
                                    output_sequence_length=None) # how long should the output sequence of tokens be?
                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None
```

This `text_vectorizer` is based on default settings.

For `max_tokens` (the number of words in the vocabulary), multiples of 10,000 (`10,000`, `20,000`, `30,000`) or the exact number of unique words in your text (e.g. `10,876`) are common values. For our use case, we'll use `10,000`.

And for the `output_sequence_length` we can use the average number of tokens per Tweet in the training set:

```python
# Find average number of tokens (words) in training Tweets
round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))
```

    15

Now let's create another `TextVectorization` object using our custom parameters.

```python
# Setup text vectorization with custom variables
max_vocab_length = 10000 # max number of words to have in our vocabulary
max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)

text_vectorizer = TextVectorization(max_tokens=max_vocab_length,
                                    output_mode="int",
                                    output_sequence_length=max_length)
```

To map our `TextVectorization` instance `text_vectorizer` to our data, we can call the `adapt()` method on it whilst passing it our training text.

```python
# Fit the text vectorizer to the training text
text_vectorizer.adapt(train_sentences)
```

```python
# For visualization purpose
# Choose a random sentence from the training dataset and tokenize it
random_sentence = random.choice(train_sentences)
print(f"Original text:\n{random_sentence}\
      \n\nVectorized version:")
text_vectorizer([random_sentence])
```

    Original text:
    @EPCOTExplorer my jealous tears are burning with the fire of a thousand ransacked buildings. SO AWESOMEEEEEEEE

    Vectorized version:





    <tf.Tensor: shape=(1, 15), dtype=int64, numpy=
    array([[   1,   13, 5320, 1460,   22,   86,   14,    2,   42,    6,    3,
            2653, 9353,   95,   28]])>

Finally, we can check the unique tokens in our vocabulary using the `get_vocabulary()` method. This method returns array sorted by frequency of each token!

```python
# Get the unique words in the vocabulary
words_in_vocab = text_vectorizer.get_vocabulary()
top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for "unknown" words)
bottom_5_words = words_in_vocab[-5:] # least common tokens
print(f"Number of words in vocab: {len(words_in_vocab)}")
print(f"Top 5 most common words: {top_5_words}")
print(f"Bottom 5 least common words: {bottom_5_words}")
```

    Number of words in vocab: 10000
    Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']
    Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']

### Creating an Embedding using an Embedding Layer

The powerful thing about an embedding is it can be learned during training. This means rather than just being static (e.g. `1` = I, `2` = love, `3` = TensorFlow), a word's numeric representation can be improved as a model goes through data samples.

We can see what an embedding of a word looks like by using the [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.

The main parameters we're concerned about here are:

- `input_dim`: The size of the vocabulary (e.g. `len(text_vectorizer.get_vocabulary()`).
- `output_dim`: The size of the output embedding vector, for example, a value of `100` outputs a feature vector of size 100 for each word.
- `embeddings_initializer`: How to initialize the embeddings matrix, default is `"uniform"` which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.
- `input_length`: Length of sequences being passed to embedding layer.

Knowing these, let's make an embedding layer.

```python
tf.random.set_seed(42)
from tensorflow.keras import layers

embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape
                             output_dim=128, # set size of embedding vector
                             embeddings_initializer="uniform", # default, intialize randomly
                             input_length=max_length, # how long is each input
                             name="embedding_1")

embedding
```

    <keras.layers.embeddings.Embedding at 0x7f4e7027de50>

Notice how `embedding` is a TensoFlow layer? This is important because we can use it as part of a model, meaning its parameters (word representations) can be updated and improved as the model learns

```python
# Get a random sentence from training set
random_sentence = random.choice(train_sentences)
print(f"Original text:\n{random_sentence}\
      \n\nEmbedded version:")

# Embed the random sentence (turn it into numerical representation)
sample_embed = embedding(text_vectorizer([random_sentence]))
sample_embed
```

    Original text:
    The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/bKsYymvIsg #GN

    Embedded version:





    <tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=
    array([[[ 0.02504804,  0.02770906,  0.01542609, ..., -0.04612003,
             -0.02924401, -0.00331711],
            [-0.01710484, -0.01808884, -0.00333623, ..., -0.04913883,
              0.03827006, -0.01127167],
            [-0.02769482, -0.02347138, -0.00921988, ..., -0.02134148,
             -0.01753038,  0.01562332],
            ...,
            [ 0.0043944 , -0.00391113, -0.03957802, ..., -0.04583538,
              0.03841076, -0.03742123],
            [ 0.01645621, -0.00589932, -0.01471175, ..., -0.02511839,
              0.00912381, -0.00024097],
            [ 0.01645621, -0.00589932, -0.01471175, ..., -0.02511839,
              0.00912381, -0.00024097]]], dtype=float32)>

Each token in the sentence gets turned into a length 128 feature vector.

```python
sample_embed[0][0]
```

    <tf.Tensor: shape=(128,), dtype=float32, numpy=
    array([ 0.02504804,  0.02770906,  0.01542609,  0.00089232, -0.00529938,
           -0.02928236,  0.00707978, -0.03920484,  0.03952933, -0.02552515,
            0.00408097,  0.00258199, -0.00851569,  0.02096765,  0.02839674,
           -0.03164236,  0.0242678 , -0.01001238,  0.01272205, -0.03405426,
           -0.00717442,  0.02189595, -0.0302518 , -0.01742091, -0.02100649,
            0.01014942,  0.03857254,  0.03433073,  0.04158163,  0.01466495,
            0.03833253, -0.01003249,  0.04828434, -0.01299055,  0.02435019,
            0.01113303,  0.0491555 ,  0.00446171, -0.00199286,  0.02555064,
            0.01129095, -0.01858953,  0.02593536, -0.04630095,  0.0032024 ,
           -0.02087005, -0.03942823,  0.01611714, -0.04888518, -0.0156971 ,
            0.00316957, -0.02797706, -0.04604458,  0.01446999, -0.02088686,
            0.04303539, -0.04312096,  0.01193186,  0.04844829,  0.04573468,
            0.0367504 , -0.04961541, -0.01993985, -0.00446255, -0.01953186,
            0.00610562, -0.03069264, -0.0252593 ,  0.0360716 ,  0.02473167,
            0.04511131, -0.01224351, -0.02150252, -0.00118319, -0.03359364,
           -0.00195736,  0.03060428,  0.00047366, -0.01751811, -0.01651092,
            0.03143331, -0.00812029, -0.02116715,  0.04153134,  0.01939831,
            0.03905955,  0.00103372, -0.00034864,  0.01314628, -0.02763629,
           -0.01169653, -0.0498125 ,  0.02969183, -0.01978601, -0.04414376,
            0.01753921,  0.01093007,  0.04583496, -0.01800694, -0.04651228,
            0.01553212, -0.03188585, -0.04447339,  0.03543087, -0.00155709,
           -0.03028064, -0.01021104,  0.03963939, -0.00155266, -0.0323783 ,
           -0.01861589, -0.0470361 , -0.03420719, -0.03797128, -0.04966434,
            0.01207765, -0.04969127, -0.01531608, -0.04282291, -0.04210033,
            0.03742603, -0.02209177,  0.04982879,  0.02247807,  0.00037289,
           -0.04612003, -0.02924401, -0.00331711], dtype=float32)>

These values might not mean much to us but they're what our computer sees each word as. When our model looks for patterns in different samples, these values will be updated as necessary.

## Modelling

### Naive Bayes (baseline)

To create our baseline, a Scikit-Learn Pipeline using the TF-IDF (term frequency-inverse document frequency) formula will be created to convert our words to numbers and then model them with the [Multinomial Naive Bayes algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB). This was chosen via referring to the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).

#### TF-IDF

$ w*{i,j} = \text{tf}*{i,j} \log \frac{N}{df_i} $

- $ \text{tf}\_{i,j} $: numberf of times $i$ occures in $j$ devided by total number of term in $j$
- $ N $: number of documents containing $i$
- $ df_i $: total number of documents

$\text{tf}_{i,j} = \frac{\text{number of occurences of a word}}{\text{number of words in text message}}$

example:

I like NLP:

$\text{tf}_{i,j} = \frac{\text{number of occurences of a NLP}}{\text{number of words in text message}} = \frac{1}{3} = 0.33$

- assume that we have 20 text massage and only we have one with NLP

$ w\_{i,j} = 0.33 \log \frac{20}{1} = 0.43 $

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

# Create tokenization and modelling pipeline
model_0 = Pipeline([
                    ("tfidf", TfidfVectorizer()), # convert words to numbers using tfidf
                    ("clf", MultinomialNB()) # model the text
])

# Fit the pipeline to the training data
model_0.fit(train_sentences, train_labels)
```

    Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])

The benefit of using a shallow model like Multinomial Naive Bayes is that training is very fast.

Let's evaluate our model and find our baseline metric.

```python
baseline_score = model_0.score(val_sentences, val_labels)
print(f"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%")
```

    Our baseline model achieves an accuracy of: 79.27%

```python
# Make predictions
baseline_preds = model_0.predict(val_sentences)
baseline_preds[:20]
```

    array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])

Evaluation of model based on:

- Accuracy
- Precision
- Recall
- F1-score

```python
# Function to evaluate: accuracy, precision, recall, f1-score
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def calculate_results(y_true, y_pred):
  """
  Calculates model accuracy, precision, recall and f1 score of a binary classification model.

  Args:
  -----
  y_true = true labels in the form of a 1D array
  y_pred = predicted labels in the form of a 1D array

  Returns a dictionary of accuracy, precision, recall, f1-score.
  """
  # Calculate model accuracy
  model_accuracy = accuracy_score(y_true, y_pred) * 100
  # Calculate model precision, recall and f1 score using "weighted" average
  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average="weighted")
  model_results = {"accuracy": model_accuracy,
                  "precision": model_precision,
                  "recall": model_recall,
                  "f1": model_f1}

  print(' Model accuracy is:', round(model_results['accuracy'], 2))
  print(' Model precision is:', round(model_results['precision'], 2))
  print(' Model recall is:', round(model_results['recall'], 2))
  print(' Model f1-score is:', round(model_results['f1'], 2))

  return model_results
```

```python
# Get baseline results
baseline_results = calculate_results(y_true=val_labels,
                                     y_pred=baseline_preds)

```

     Model accuracy is: 79.27
     Model precision is: 0.81
     Model recall is: 0.79
     Model f1-score is: 0.79

### Feed-forward neural network (dense model)

The first "deep" model we're going to build is a single layer dense model.

We already defined `TextVectorization` and `Embedding` layers. To make example end-to-end, we just put definition of these two layers again in this cell:

```python
max_vocab_length = 10000 # max number of words to have in our vocabulary
max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)

text_vectorizer = TextVectorization(max_tokens=max_vocab_length,
                                    output_mode="int",
                                    output_sequence_length=max_length)
# Fit the text vectorizer to the training text
text_vectorizer.adapt(train_sentences)


embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape
                             output_dim=128, # set size of embedding vector
                             embeddings_initializer="uniform", # default, intialize randomly
                             input_length=max_length, # how long is each input
                             name="embedding_1")
```

```python
# Create directory to save TensorBoard logs
SAVE_DIR = "model_logs"

# Build model with the Functional API - more customizable than PyTorch
from tensorflow.keras import layers
inputs = layers.Input(shape=(1,), dtype="string") # inputs are 1-dimensional strings
x = text_vectorizer(inputs) # turn the input text into numbers
x = embedding(x) # create an embedding of the numerized numbers
x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding (try running the model without this layer and see what happens)
outputs = layers.Dense(1, activation="sigmoid")(x) # create the output layer, want binary outputs so use sigmoid activation
model_1 = tf.keras.Model(inputs, outputs, name="model_1_FF_dense") # construct the model
```

Looking good. Our model takes a 1-dimensional string as input (in our case, a Tweet), it then tokenizes the string using `text_vectorizer` and creates an embedding using `embedding`.

We then (optionally) pool the outputs of the embedding layer to reduce the dimensionality of the tensor we pass to the output layer.

Finally, we pass the output of the pooling layer to a dense layer with sigmoid activation (we use sigmoid since our problem is binary classification).

Before we can fit our model to the data, we've got to compile it. Since we're working with binary classification, we'll use `"binary_crossentropy"` as our loss function and the Adam optimizer.

```python
# Compile model
model_1.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])
```

```python
# Get a summary of the model
model_1.summary()
```

    Model: "model_1_FF_dense"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #
    =================================================================
     input_1 (InputLayer)        [(None, 1)]               0

     text_vectorization_2 (TextV  (None, 15)               0
     ectorization)

     embedding_1 (Embedding)     (None, 15, 128)           1280000

     global_average_pooling1d (G  (None, 128)              0
     lobalAveragePooling1D)

     dense (Dense)               (None, 1)                 129

    =================================================================
    Total params: 1,280,129
    Trainable params: 1,280,129
    Non-trainable params: 0
    _________________________________________________________________

Most of the trainable parameters are contained within the embedding layer. Recall we created an embedding of size 128 (`output_dim=128`) for a vocabulary of size 10,000 (`input_dim=10000`), hence the 1,280,000 trainable parameters.

```python

# Fit the model
model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model
                              train_labels,
                              epochs=15,
                              validation_data=(val_sentences, val_labels),
                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,
                                                                     experiment_name="simple_dense_model")])
```

    Saving TensorBoard log files to: model_logs/simple_dense_model/20220908-205523
    Epoch 1/15
    215/215 [==============================] - 8s 11ms/step - loss: 0.6097 - accuracy: 0.6929 - val_loss: 0.5364 - val_accuracy: 0.7559
    Epoch 2/15
    215/215 [==============================] - 2s 9ms/step - loss: 0.4414 - accuracy: 0.8189 - val_loss: 0.4694 - val_accuracy: 0.7835
    Epoch 3/15
    215/215 [==============================] - 2s 7ms/step - loss: 0.3467 - accuracy: 0.8600 - val_loss: 0.4591 - val_accuracy: 0.7887
    Epoch 4/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.2852 - accuracy: 0.8921 - val_loss: 0.4640 - val_accuracy: 0.7887
    Epoch 5/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.2383 - accuracy: 0.9126 - val_loss: 0.4764 - val_accuracy: 0.7874
    Epoch 6/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.2021 - accuracy: 0.9282 - val_loss: 0.4993 - val_accuracy: 0.7861
    Epoch 7/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.1727 - accuracy: 0.9399 - val_loss: 0.5262 - val_accuracy: 0.7861
    Epoch 8/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.1500 - accuracy: 0.9458 - val_loss: 0.5544 - val_accuracy: 0.7782
    Epoch 9/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.1305 - accuracy: 0.9556 - val_loss: 0.5970 - val_accuracy: 0.7861
    Epoch 10/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.1164 - accuracy: 0.9607 - val_loss: 0.6173 - val_accuracy: 0.7769
    Epoch 11/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.1045 - accuracy: 0.9645 - val_loss: 0.6393 - val_accuracy: 0.7769
    Epoch 12/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0944 - accuracy: 0.9695 - val_loss: 0.6743 - val_accuracy: 0.7730
    Epoch 13/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0860 - accuracy: 0.9712 - val_loss: 0.7064 - val_accuracy: 0.7756
    Epoch 14/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0795 - accuracy: 0.9730 - val_loss: 0.7571 - val_accuracy: 0.7664
    Epoch 15/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0749 - accuracy: 0.9749 - val_loss: 0.7720 - val_accuracy: 0.7756

```python
plot_loss_curves(model_1_history)
```

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_69_0.png)

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_69_1.png)

```python
# Check the results
model_1.evaluate(val_sentences, val_labels)
```

    24/24 [==============================] - 0s 3ms/step - loss: 0.7720 - accuracy: 0.7756





    [0.7720077633857727, 0.7755905389785767]

```python
# Make predictions (these come back in the form of probabilities)
model_1_pred_probs = model_1.predict(val_sentences)
model_1_pred_probs[:10] # only print out the first 10 prediction probabilities
```

    array([[0.6019999 ],
           [0.7605659 ],
           [0.9991002 ],
           [0.11624473],
           [0.00158106],
           [0.9898642 ],
           [0.85501844],
           [0.9999776 ],
           [0.9999572 ],
           [0.80038756]], dtype=float32)

Since our final layer uses a sigmoid activation function, we get our predictions back in the form of probabilities.

To convert them to prediction classes, we'll use `tf.round()`, meaning prediction probabilities below 0.5 will be rounded to 0 and those above 0.5 will be rounded to 1.

> In practice, the output threshold of a sigmoid prediction probability doesn't necessarily have to 0.5. For example, through testing, you may find that a cut off of 0.25 is better for your chosen evaluation metrics. A common example of this threshold cutoff is the [precision-recall tradeoff](https://www.machinelearningaptitude.com/topics/machine-learning/what-is-precision-recall-tradeoff/#:~:text=precision%2Drecall%20tradeoff%20occur%20due,the%20threshold%20of%20the%20classifier.&text=When%20threshold%20is%20decreased%20to,but%20precision%20decreases%20to%200.4.).

```python
# Turn prediction probabilities into single-dimension tensor of floats
model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions
model_1_preds[:20]
```

    <tf.Tensor: shape=(20,), dtype=float32, numpy=
    array([1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
           0., 0., 1.], dtype=float32)>

Now we've got our model's predictions in the form of classes, we can use our calculate_results() function to compare them to the ground truth validation labels.

```python
# Calculate model_1 metrics
model_1_results = calculate_results(y_true=val_labels,
                                    y_pred=model_1_preds)
```

     Model accuracy is: 77.56
     Model precision is: 0.78
     Model recall is: 0.78
     Model f1-score is: 0.77

```python
# Create a helper function to compare our baseline results to new model results
def compare_baseline_to_new_results(baseline_results, new_model_results):
  for key, value in baseline_results.items():
    print(f"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}")


compare_baseline_to_new_results(baseline_results=baseline_results,
                                new_model_results=model_1_results)
```

    Baseline accuracy: 79.27, New accuracy: 77.56, Difference: -1.71
    Baseline precision: 0.81, New precision: 0.78, Difference: -0.04
    Baseline recall: 0.79, New recall: 0.78, Difference: -0.02
    Baseline f1: 0.79, New f1: 0.77, Difference: -0.01

#### Visualizing learned embeddings

Our first model (`model_1`) contained an embedding layer (`embedding`) which learned a way of representing words as feature vectors by passing over the training data.

we can use the [Embedding Projector tool](http://projector.tensorflow.org/) to visualize embedding layer.

To use the Embedding Projector tool, we need two files:

- The embedding vectors (same as embedding weights).
- The meta data of the embedding vectors (the words they represent - our vocabulary).

```python
# Get the vocabulary from the text vectorization layer
words_in_vocab = text_vectorizer.get_vocabulary()
len(words_in_vocab), words_in_vocab[:10]

```

    (10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])

```python

# Get the weight matrix of embedding layer
# (these are the numerical patterns between the text in the training dataset the model has learned)
embed_weights = model_1.get_layer("embedding_1").get_weights()[0]
print(embed_weights.shape) # same size as vocab size and embedding_dim (each word is a embedding_dim size vector)
```

    (10000, 128)

```python
import io

# Create output writers
out_v = io.open("embedding_vectors.tsv", "w", encoding="utf-8")
out_m = io.open("embedding_metadata.tsv", "w", encoding="utf-8")

# Write embedding vectors and words to file
for num, word in enumerate(words_in_vocab):
  if num == 0:
     continue # skip padding token
  vec = embed_weights[num]
  out_m.write(word + "\n") # write words to file
  out_v.write("\t".join([str(x) for x in vec]) + "\n") # write corresponding word vector to file
out_v.close()
out_m.close()

# Download files locally to upload to Embedding Projector
try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download("embedding_vectors.tsv")
  files.download("embedding_metadata.tsv")
```

    <IPython.core.display.Javascript object>



    <IPython.core.display.Javascript object>



    <IPython.core.display.Javascript object>



    <IPython.core.display.Javascript object>

### Recurrent Neural Networks (RNN's)

For our next series of modelling experiments we're going to be using a special kind of neural network called a **Recurrent Neural Network (RNN)**.

The premise of an RNN is simple: use information from the past to help you with the future (this is where the term recurrent comes from). In other words, take an input (`X`) and compute an output (`y`) based on all previous inputs.

This concept is especially helpful when dealing with sequences such as passages of natural language text (such as our Tweets).

For example, when you read this sentence, you take into context the previous words when deciphering the meaning of the current word dog.

Recurrent neural networks can be used for a number of sequence-based problems:

- **One to one:** one input, one output, such as image classification.
- **One to many:** one input, many outputs, such as image captioning (image input, a sequence of text as caption output).
- **Many to one:** many inputs, one outputs, such as text classification (classifying a Tweet as real diaster or not real diaster).
- **Many to many:** many inputs, many outputs, such as machine translation (translating English to Spanish) or speech to text (audio wave as input, text as output).

When you come across RNN's in the wild, you'll most likely come across variants of the following:

- Long short-term memory cells (LSTMs).
- Gated recurrent units (GRUs).
- Bidirectional RNN's (passes forward and backward along a sequence, left to right and right to left).

Going into the details of each these is beyond the scope of this notebook (we're going to focus on using them instead), the main thing you should know for now is that they've proven very effective at modelling sequences.

#### Long short-term memory cells (LSTMs) model

To harness the power of the LSTM cell (LSTM cell and LSTM layer are often used interchangably) in TensorFlow, we will use [`tensorflow.keras.layers.LSTM()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM).

Our model is going to take on a very similar structure to `model_1`:

```
Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)
```

The main difference will be that we are going to add an LSTM layer between our embedding and output.

To avoid data leakage between models, another embedding layer (`model_2_embedding`) for LSTM model will be created. The `text_vectorizer` layer can be reused since it doesn't get updated during training.

```python
# Set random seed and create embedding layer (new embedding layer for each model)
tf.random.set_seed(42)

model_2_embedding = layers.Embedding(input_dim=max_vocab_length,
                                     output_dim=128,
                                     embeddings_initializer="uniform",
                                     input_length=max_length,
                                     name="embedding_2")


# Create LSTM model
inputs = layers.Input(shape=(1,), dtype="string")
x = text_vectorizer(inputs)
x = model_2_embedding(x)
print(x.shape)
# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)
x = layers.LSTM(64)(x) # return vector for whole sequence
print(x.shape)
# x = layers.Dense(64, activation="relu")(x) # optional dense layer on top of output of LSTM cell
outputs = layers.Dense(1, activation="sigmoid")(x)
model_2 = tf.keras.Model(inputs, outputs, name="model_2_LSTM")
```

    (None, 15, 128)
    (None, 64)

Reading the documentation for the [TensorFlow LSTM layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), you'll find a plethora of parameters. Many of these have been tuned to make sure they compute as fast as possible. The main ones you'll be looking to adjust are `units` (number of hidden units) and `return_sequences` (set this to `True` when stacking LSTM or other recurrent layers).

Now we've got our LSTM model built, let's compile it using `"binary_crossentropy"` loss and the Adam optimizer.

```python
# Compile model
model_2.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

model_2.summary()
```

    Model: "model_2_LSTM"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #
    =================================================================
     input_2 (InputLayer)        [(None, 1)]               0

     text_vectorization_2 (TextV  (None, 15)               0
     ectorization)

     embedding_2 (Embedding)     (None, 15, 128)           1280000

     lstm (LSTM)                 (None, 64)                49408

     dense_1 (Dense)             (None, 1)                 65

    =================================================================
    Total params: 1,329,473
    Trainable params: 1,329,473
    Non-trainable params: 0
    _________________________________________________________________

Now our first RNN model's compiled let's fit it to our training data, validating it on the validation data and tracking its training parameters using our TensorBoard callback.

```python
# Fit model
model_2_history = model_2.fit(train_sentences,
                              train_labels,
                              epochs=15,
                              validation_data=(val_sentences, val_labels),
                              callbacks=[create_tensorboard_callback(SAVE_DIR,
                                                                     "LSTM")])
```

    Saving TensorBoard log files to: model_logs/LSTM/20220908-205550
    Epoch 1/15
    215/215 [==============================] - 6s 8ms/step - loss: 0.5100 - accuracy: 0.7416 - val_loss: 0.4566 - val_accuracy: 0.7822
    Epoch 2/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.3176 - accuracy: 0.8717 - val_loss: 0.5138 - val_accuracy: 0.7756
    Epoch 3/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.2201 - accuracy: 0.9152 - val_loss: 0.5858 - val_accuracy: 0.7677
    Epoch 4/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.1556 - accuracy: 0.9428 - val_loss: 0.6041 - val_accuracy: 0.7743
    Epoch 5/15
    215/215 [==============================] - 1s 7ms/step - loss: 0.1076 - accuracy: 0.9594 - val_loss: 0.8746 - val_accuracy: 0.7507
    Epoch 6/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0856 - accuracy: 0.9641 - val_loss: 0.9424 - val_accuracy: 0.7638
    Epoch 7/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0720 - accuracy: 0.9698 - val_loss: 1.0568 - val_accuracy: 0.7585
    Epoch 8/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0574 - accuracy: 0.9747 - val_loss: 1.0869 - val_accuracy: 0.7612
    Epoch 9/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0558 - accuracy: 0.9758 - val_loss: 1.2186 - val_accuracy: 0.7572
    Epoch 10/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0500 - accuracy: 0.9765 - val_loss: 1.2605 - val_accuracy: 0.7651
    Epoch 11/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0462 - accuracy: 0.9787 - val_loss: 1.2471 - val_accuracy: 0.7664
    Epoch 12/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0394 - accuracy: 0.9799 - val_loss: 1.1651 - val_accuracy: 0.7690
    Epoch 13/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0531 - accuracy: 0.9777 - val_loss: 1.3284 - val_accuracy: 0.7677
    Epoch 14/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0423 - accuracy: 0.9783 - val_loss: 1.3699 - val_accuracy: 0.7612
    Epoch 15/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0437 - accuracy: 0.9794 - val_loss: 1.4239 - val_accuracy: 0.7612

```python
plot_loss_curves(model_2_history)
```

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_90_0.png)

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_90_1.png)

```python
# Make predictions on the validation dataset
model_2_pred_probs = model_2.predict(val_sentences)
model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10
```

    ((762, 1), array([[9.4284089e-03],
            [7.6338172e-01],
            [9.9997652e-01],
            [3.8744417e-01],
            [4.8809958e-05],
            [9.9995196e-01],
            [9.9742907e-01],
            [9.9998879e-01],
            [9.9997950e-01],
            [1.7864858e-01]], dtype=float32))

We can turn these prediction probabilities into prediction classes by rounding to the nearest integer (by default, prediction probabilities under 0.5 will go to 0 and those over 0.5 will go to 1).

```python
# Round out predictions and reduce to 1-dimensional array
model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))
model_2_preds[:10]
```

    <tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>

```python
# Calculate LSTM model results
model_2_results = calculate_results(y_true=val_labels,
                                    y_pred=model_2_preds)
```

     Model accuracy is: 76.12
     Model precision is: 0.76
     Model recall is: 0.76
     Model f1-score is: 0.76

```python
# Compare model 2 to baseline
compare_baseline_to_new_results(baseline_results, model_2_results)
```

    Baseline accuracy: 79.27, New accuracy: 76.12, Difference: -3.15
    Baseline precision: 0.81, New precision: 0.76, Difference: -0.05
    Baseline recall: 0.79, New recall: 0.76, Difference: -0.03
    Baseline f1: 0.79, New f1: 0.76, Difference: -0.03

#### Gated Recurrent Units (GRU) model

### Model 3: GRU

Another popular and effective RNN component is the GRU or gated recurrent unit.

The GRU cell has similar features to an LSTM cell but has less parameters.

The architecture of the GRU-powered model will follow the same structure we've been using:

```
Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)
```

Again, the only difference will be the layer(s) we use between the embedding and the output.

```python
# Set random seed and create embedding layer (new embedding layer for each model)
tf.random.set_seed(42)
from tensorflow.keras import layers
model_3_embedding = layers.Embedding(input_dim=max_vocab_length,
                                     output_dim=128,
                                     embeddings_initializer="uniform",
                                     input_length=max_length,
                                     name="embedding_3")

# Build an RNN using the GRU cell
inputs = layers.Input(shape=(1,), dtype="string")
x = text_vectorizer(inputs)
x = model_3_embedding(x)
# x = layers.GRU(64, return_sequences=True) # stacking recurrent cells requires return_sequences=True
x = layers.GRU(64)(x)
# x = layers.Dense(64, activation="relu")(x) # optional dense layer after GRU cell
outputs = layers.Dense(1, activation="sigmoid")(x)
model_3 = tf.keras.Model(inputs, outputs, name="model_3_GRU")
```

```python
# Compile GRU model
model_3.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

# Get a summary of the GRU model
model_3.summary()
```

    Model: "model_3_GRU"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #
    =================================================================
     input_3 (InputLayer)        [(None, 1)]               0

     text_vectorization_2 (TextV  (None, 15)               0
     ectorization)

     embedding_3 (Embedding)     (None, 15, 128)           1280000

     gru (GRU)                   (None, 64)                37248

     dense_2 (Dense)             (None, 1)                 65

    =================================================================
    Total params: 1,317,313
    Trainable params: 1,317,313
    Non-trainable params: 0
    _________________________________________________________________

Notice the difference in number of trainable parameters between `model_2` (LSTM) and `model_3` (GRU). The difference comes from the LSTM cell having more trainable parameters than the GRU cell.

We'll fit our model just as we've been doing previously. We'll also track our models results using our `create_tensorboard_callback()` function.

```python
# Fit model
model_3_history = model_3.fit(train_sentences,
                              train_labels,
                              epochs=15,
                              validation_data=(val_sentences, val_labels),
                              callbacks=[create_tensorboard_callback(SAVE_DIR, "GRU")])
```

    Saving TensorBoard log files to: model_logs/GRU/20220908-205634
    Epoch 1/15
    215/215 [==============================] - 3s 8ms/step - loss: 0.5242 - accuracy: 0.7314 - val_loss: 0.4553 - val_accuracy: 0.7769
    Epoch 2/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.3195 - accuracy: 0.8694 - val_loss: 0.4937 - val_accuracy: 0.7808
    Epoch 3/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.2197 - accuracy: 0.9181 - val_loss: 0.5607 - val_accuracy: 0.7743
    Epoch 4/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.1599 - accuracy: 0.9441 - val_loss: 0.6220 - val_accuracy: 0.7782
    Epoch 5/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.1221 - accuracy: 0.9584 - val_loss: 0.6205 - val_accuracy: 0.7677
    Epoch 6/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.1009 - accuracy: 0.9654 - val_loss: 0.7155 - val_accuracy: 0.7638
    Epoch 7/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0797 - accuracy: 0.9723 - val_loss: 0.8096 - val_accuracy: 0.7651
    Epoch 8/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0631 - accuracy: 0.9753 - val_loss: 1.0708 - val_accuracy: 0.7612
    Epoch 9/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0543 - accuracy: 0.9772 - val_loss: 1.1966 - val_accuracy: 0.7546
    Epoch 10/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0504 - accuracy: 0.9784 - val_loss: 1.0911 - val_accuracy: 0.7598
    Epoch 11/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0484 - accuracy: 0.9788 - val_loss: 1.1171 - val_accuracy: 0.7717
    Epoch 12/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0452 - accuracy: 0.9791 - val_loss: 1.3732 - val_accuracy: 0.7559
    Epoch 13/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0446 - accuracy: 0.9788 - val_loss: 1.4159 - val_accuracy: 0.7533
    Epoch 14/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0409 - accuracy: 0.9809 - val_loss: 1.4167 - val_accuracy: 0.7612
    Epoch 15/15
    215/215 [==============================] - 1s 6ms/step - loss: 0.0383 - accuracy: 0.9813 - val_loss: 1.4121 - val_accuracy: 0.7480

```python
plot_loss_curves(model_3_history)
```

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_102_0.png)

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_102_1.png)

```python
# Make predictions on the validation data
model_3_pred_probs = model_3.predict(val_sentences)
model_3_pred_probs.shape, model_3_pred_probs[:10]
```

    ((762, 1), array([[9.2305779e-01],
            [9.0807080e-01],
            [9.9993038e-01],
            [3.0870089e-01],
            [2.5155299e-04],
            [9.9995625e-01],
            [9.4583201e-01],
            [9.9997258e-01],
            [9.9996185e-01],
            [1.8717250e-01]], dtype=float32))

```python
# Convert prediction probabilities to prediction classes
model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))
model_3_preds[:10]
```

    <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>

```python
# Calcuate model_3 results
model_3_results = calculate_results(y_true=val_labels,
                                    y_pred=model_3_preds)
```

     Model accuracy is: 74.8
     Model precision is: 0.75
     Model recall is: 0.75
     Model f1-score is: 0.75

```python
# Compare to baseline
compare_baseline_to_new_results(baseline_results, model_3_results)
```

    Baseline accuracy: 79.27, New accuracy: 74.80, Difference: -4.46
    Baseline precision: 0.81, New precision: 0.75, Difference: -0.06
    Baseline recall: 0.79, New recall: 0.75, Difference: -0.04
    Baseline f1: 0.79, New f1: 0.75, Difference: -0.04

#### Bidirectional-LSTM model

A standard RNN will process a sequence from left to right, where as a bidirectional RNN will process the sequence from left to right and then again from right to left.

Intuitively, this can be thought of as if you were reading a sentence for the first time in the normal fashion (left to right) but for some reason it didn't make sense so you traverse back through the words and go back over them again (right to left).

In practice, many sequence models often see and improvement in performance when using bidirectional RNN's.

However, this improvement in performance often comes at the cost of longer training times and increased model parameters (since the model goes left to right and right to left, the number of trainable parameters doubles).

TensorFlow helps us out by providing the [`tensorflow.keras.layers.Bidirectional`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) class. We can use the `Bidirectional` class to wrap our existing RNNs, instantly making them bidirectional.

```python
# Set random seed and create embedding layer (new embedding layer for each model)
tf.random.set_seed(42)
from tensorflow.keras import layers
model_4_embedding = layers.Embedding(input_dim=max_vocab_length,
                                     output_dim=128,
                                     embeddings_initializer="uniform",
                                     input_length=max_length,
                                     name="embedding_4")

# Build a Bidirectional RNN in TensorFlow
inputs = layers.Input(shape=(1,), dtype="string")
x = text_vectorizer(inputs)
x = model_4_embedding(x)
# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True
x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer
outputs = layers.Dense(1, activation="sigmoid")(x)
model_4 = tf.keras.Model(inputs, outputs, name="model_4_Bidirectional")
```

> **Note:** You can use the `Bidirectional` wrapper on any RNN cell in TensorFlow. For example, `layers.Bidirectional(layers.GRU(64))` creates a bidirectional GRU cell.

```python
# Compile
model_4.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

# Get a summary of our bidirectional model
model_4.summary()
```

    Model: "model_4_Bidirectional"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #
    =================================================================
     input_4 (InputLayer)        [(None, 1)]               0

     text_vectorization_2 (TextV  (None, 15)               0
     ectorization)

     embedding_4 (Embedding)     (None, 15, 128)           1280000

     bidirectional (Bidirectiona  (None, 128)              98816
     l)

     dense_3 (Dense)             (None, 1)                 129

    =================================================================
    Total params: 1,378,945
    Trainable params: 1,378,945
    Non-trainable params: 0
    _________________________________________________________________

```python
# Fit the model (takes longer because of the bidirectional layers)
model_4_history = model_4.fit(train_sentences,
                              train_labels,
                              epochs=15,
                              validation_data=(val_sentences, val_labels),
                              callbacks=[create_tensorboard_callback(SAVE_DIR, "bidirectional_RNN")])
```

    Saving TensorBoard log files to: model_logs/bidirectional_RNN/20220908-205657
    Epoch 1/15
    215/215 [==============================] - 6s 12ms/step - loss: 0.5093 - accuracy: 0.7481 - val_loss: 0.4606 - val_accuracy: 0.7795
    Epoch 2/15
    215/215 [==============================] - 2s 9ms/step - loss: 0.3135 - accuracy: 0.8708 - val_loss: 0.5144 - val_accuracy: 0.7690
    Epoch 3/15
    215/215 [==============================] - 2s 9ms/step - loss: 0.2150 - accuracy: 0.9178 - val_loss: 0.5626 - val_accuracy: 0.7677
    Epoch 4/15
    215/215 [==============================] - 2s 9ms/step - loss: 0.1523 - accuracy: 0.9469 - val_loss: 0.6365 - val_accuracy: 0.7769
    Epoch 5/15
    215/215 [==============================] - 2s 10ms/step - loss: 0.1083 - accuracy: 0.9639 - val_loss: 0.6509 - val_accuracy: 0.7664
    Epoch 6/15
    215/215 [==============================] - 4s 19ms/step - loss: 0.0856 - accuracy: 0.9699 - val_loss: 0.7892 - val_accuracy: 0.7520
    Epoch 7/15
    215/215 [==============================] - 3s 13ms/step - loss: 0.0639 - accuracy: 0.9753 - val_loss: 0.9846 - val_accuracy: 0.7598
    Epoch 8/15
    215/215 [==============================] - 2s 8ms/step - loss: 0.0501 - accuracy: 0.9784 - val_loss: 1.1654 - val_accuracy: 0.7651
    Epoch 9/15
    215/215 [==============================] - 2s 8ms/step - loss: 0.0453 - accuracy: 0.9784 - val_loss: 0.8952 - val_accuracy: 0.7598
    Epoch 10/15
    215/215 [==============================] - 2s 8ms/step - loss: 0.0466 - accuracy: 0.9790 - val_loss: 1.2571 - val_accuracy: 0.7559
    Epoch 11/15
    215/215 [==============================] - 2s 8ms/step - loss: 0.0387 - accuracy: 0.9806 - val_loss: 1.2990 - val_accuracy: 0.7598
    Epoch 12/15
    215/215 [==============================] - 2s 8ms/step - loss: 0.0361 - accuracy: 0.9816 - val_loss: 1.4568 - val_accuracy: 0.7546
    Epoch 13/15
    215/215 [==============================] - 2s 8ms/step - loss: 0.0410 - accuracy: 0.9812 - val_loss: 1.2968 - val_accuracy: 0.7533
    Epoch 14/15
    215/215 [==============================] - 2s 8ms/step - loss: 0.0382 - accuracy: 0.9828 - val_loss: 1.3390 - val_accuracy: 0.7677
    Epoch 15/15
    215/215 [==============================] - 2s 8ms/step - loss: 0.0386 - accuracy: 0.9832 - val_loss: 1.2691 - val_accuracy: 0.7677

```python
plot_loss_curves(model_4_history)
```

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_113_0.png)

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_113_1.png)

```python
# Make predictions with bidirectional RNN on the validation data
model_4_pred_probs = model_4.predict(val_sentences)
model_4_pred_probs[:10]
```

    array([[5.6212366e-04],
           [7.6725042e-01],
           [9.9994290e-01],
           [1.4688976e-01],
           [2.6282665e-05],
           [9.9900824e-01],
           [9.9752778e-01],
           [9.9997425e-01],
           [9.9998677e-01],
           [2.1815849e-02]], dtype=float32)

```python
# Convert prediction probabilities to labels
model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))
model_4_preds[:10]
```

    <tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>

```python
# Calculate bidirectional RNN model results
model_4_results = calculate_results(val_labels, model_4_preds)
```

     Model accuracy is: 76.77
     Model precision is: 0.77
     Model recall is: 0.77
     Model f1-score is: 0.77

```python
# Check to see how the bidirectional model performs against the baseline
compare_baseline_to_new_results(baseline_results, model_4_results)
```

    Baseline accuracy: 79.27, New accuracy: 76.77, Difference: -2.49
    Baseline precision: 0.81, New precision: 0.77, Difference: -0.04
    Baseline recall: 0.79, New recall: 0.77, Difference: -0.02
    Baseline f1: 0.79, New f1: 0.77, Difference: -0.02

### 1D Convolutional Neural Network

The main difference between using CNNs for images and sequences is the shape of the data. Images come in 2-dimensions (height x width) where as sequences are often 1-dimensional (a string of text).

So to use CNNs with sequences, we use a 1-dimensional convolution instead of a 2-dimensional convolution.

A typical CNN architecture for sequences will look like the following:

```
Inputs (text) -> Tokenization -> Embedding -> Layers -> Outputs (class probabilities)
```

**Resource:** The intuition here is explained succinctly in the paper [_Understanding Convolutional Neural Networks for Text Classification_](https://www.aclweb.org/anthology/W18-5408.pdf), where they state that CNNs classify text through the following steps:

1. 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams (an ngram is a collection of n-words, for example, an ngram of 5 might result in "hello, my name is Daniel").
2. Max-pooling over time extracts the relevant ngrams for making a decision.
3. The rest of the network classifies the text based on this information.

First an embedding of a sample of text will be created, the we will experiment passing it through a `Conv1D()` layer and `GlobalMaxPool1D()` layer.

```python
# Test out the embedding, 1D convolutional and max pooling
embedding_test = embedding(text_vectorizer(["this is a test sentence"])) # turn target sentence into embedding

conv_1d = layers.Conv1D(filters=32, kernel_size=5, activation="relu") # convolve over target sequence 5 words at a time
conv_1d_output = conv_1d(embedding_test) # pass embedding through 1D convolutional layer

max_pool = layers.GlobalMaxPool1D()
max_pool_output = max_pool(conv_1d_output) # get the most important features

embedding_test.shape, conv_1d_output.shape, max_pool_output.shape
```

    (TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))

Notice the output shapes of each layer.

The embedding has an output shape dimension of the parameters we set it to (`input_length=15` and `output_dim=128`).

The 1-dimensional convolutional layer has an output which has been compressed inline with its parameters. And the same goes for the max pooling layer output.

Our text starts out as a string but gets converted to a feature vector of length 64 through various transformation steps (from tokenization to embedding to 1-dimensional convolution to max pool).

```python
# Set random seed and create embedding layer (new embedding layer for each model)
tf.random.set_seed(42)
from tensorflow.keras import layers
model_5_embedding = layers.Embedding(input_dim=max_vocab_length,
                                     output_dim=128,
                                     embeddings_initializer="uniform",
                                     input_length=max_length,
                                     name="embedding_5")

# Create 1-dimensional convolutional layer to model sequences
from tensorflow.keras import layers
inputs = layers.Input(shape=(1,), dtype="string")
x = text_vectorizer(inputs)
x = model_5_embedding(x)
x = layers.Conv1D(filters=32, kernel_size=5, activation="relu")(x)
x = layers.GlobalMaxPool1D()(x)
# x = layers.Dense(64, activation="relu")(x) # optional dense layer
outputs = layers.Dense(1, activation="sigmoid")(x)
model_5 = tf.keras.Model(inputs, outputs, name="model_5_Conv1D")

# Compile Conv1D model
model_5.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

# Get a summary of our 1D convolution model
model_5.summary()
```

    Model: "model_5_Conv1D"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #
    =================================================================
     input_5 (InputLayer)        [(None, 1)]               0

     text_vectorization_2 (TextV  (None, 15)               0
     ectorization)

     embedding_5 (Embedding)     (None, 15, 128)           1280000

     conv1d_1 (Conv1D)           (None, 11, 32)            20512

     global_max_pooling1d_1 (Glo  (None, 32)               0
     balMaxPooling1D)

     dense_4 (Dense)             (None, 1)                 33

    =================================================================
    Total params: 1,300,545
    Trainable params: 1,300,545
    Non-trainable params: 0
    _________________________________________________________________

```python
# Fit the model
model_5_history = model_5.fit(train_sentences,
                              train_labels,
                              epochs=15,
                              validation_data=(val_sentences, val_labels),
                              callbacks=[create_tensorboard_callback(SAVE_DIR,
                                                                     "Conv1D")])
```

    Saving TensorBoard log files to: model_logs/Conv1D/20220908-205749
    Epoch 1/15
    215/215 [==============================] - 3s 7ms/step - loss: 0.5652 - accuracy: 0.7141 - val_loss: 0.4733 - val_accuracy: 0.7795
    Epoch 2/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.3380 - accuracy: 0.8615 - val_loss: 0.4758 - val_accuracy: 0.7730
    Epoch 3/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.2070 - accuracy: 0.9234 - val_loss: 0.5457 - val_accuracy: 0.7730
    Epoch 4/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.1314 - accuracy: 0.9578 - val_loss: 0.6163 - val_accuracy: 0.7730
    Epoch 5/15
    215/215 [==============================] - 2s 9ms/step - loss: 0.0933 - accuracy: 0.9691 - val_loss: 0.6779 - val_accuracy: 0.7782
    Epoch 6/15
    215/215 [==============================] - 2s 9ms/step - loss: 0.0829 - accuracy: 0.9714 - val_loss: 0.7070 - val_accuracy: 0.7690
    Epoch 7/15
    215/215 [==============================] - 2s 7ms/step - loss: 0.0663 - accuracy: 0.9759 - val_loss: 0.7399 - val_accuracy: 0.7743
    Epoch 8/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0642 - accuracy: 0.9746 - val_loss: 0.7591 - val_accuracy: 0.7795
    Epoch 9/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0551 - accuracy: 0.9787 - val_loss: 0.7925 - val_accuracy: 0.7782
    Epoch 10/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0538 - accuracy: 0.9781 - val_loss: 0.8114 - val_accuracy: 0.7900
    Epoch 11/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0516 - accuracy: 0.9794 - val_loss: 0.7883 - val_accuracy: 0.7703
    Epoch 12/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0486 - accuracy: 0.9793 - val_loss: 0.8009 - val_accuracy: 0.7730
    Epoch 13/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0474 - accuracy: 0.9801 - val_loss: 0.8062 - val_accuracy: 0.7677
    Epoch 14/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0461 - accuracy: 0.9801 - val_loss: 0.8719 - val_accuracy: 0.7782
    Epoch 15/15
    215/215 [==============================] - 1s 5ms/step - loss: 0.0443 - accuracy: 0.9809 - val_loss: 0.8312 - val_accuracy: 0.7703

```python
plot_loss_curves(model_5_history)
```

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_126_0.png)

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_126_1.png)

```python
# Make predictions with model_5
model_5_pred_probs = model_5.predict(val_sentences)
model_5_pred_probs[:10]
```

    array([[0.05745949],
           [0.9398075 ],
           [0.9998342 ],
           [0.24426804],
           [0.01454076],
           [0.9894583 ],
           [0.96469593],
           [0.99957556],
           [0.9997074 ],
           [0.0700726 ]], dtype=float32)

```python
# Convert model_5 prediction probabilities to labels
model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))
model_5_preds[:10]
```

    <tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>

```python
# Calculate model_5 evaluation metrics
model_5_results = calculate_results(y_true=val_labels,
                                    y_pred=model_5_preds)
```

     Model accuracy is: 77.03
     Model precision is: 0.77
     Model recall is: 0.77
     Model f1-score is: 0.77

```python
# Compare model_5 results to baseline
compare_baseline_to_new_results(baseline_results, model_5_results)
```

    Baseline accuracy: 79.27, New accuracy: 77.03, Difference: -2.23
    Baseline precision: 0.81, New precision: 0.77, Difference: -0.04
    Baseline recall: 0.79, New recall: 0.77, Difference: -0.02
    Baseline f1: 0.79, New f1: 0.77, Difference: -0.02

### Using Pretrained Embeddings (transfer learning for NLP)

For all of the previous deep learning models we have built and trained, we havecreated and used our own embeddings from scratch each time.

However, a common practice is to leverage pretrained embeddings through **transfer learning**. This is one of the main benefits of using deep models: being able to take what one (often larger) model has learned (often on a large amount of data) and adjust it for our own use case.

In this section [Universal Sentence Encoder](https://www.aclweb.org/anthology/D18-2029.pdf) from [TensorFlow Hub](https://tfhub.dev/google/universal-sentence-encoder/4) will be used.

#### TensorFlow Hub Pretrained Feature Extractor: Universal Sentence Encoder

The main difference between the embedding layer we created and the Universal Sentence Encoder is that rather than create a word-level embedding, the Universal Sentence Encoder, as you might've guessed, creates a whole sentence-level embedding.

Our embedding layer also outputs an a 128 dimensional vector for each word, where as, the Universal Sentence Encoder outputs a 512 dimensional vector for each sentence.

We can load in a TensorFlow Hub module using the [`hub.load()`](https://www.tensorflow.org/hub/api_docs/python/hub/load) method and passing it the target URL of the module we'd like to use, in our case, it is "https://tfhub.dev/google/universal-sentence-encoder/4".

```python
# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4
import tensorflow_hub as hub
sample_sentence = "There's a flood in my street!"
embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4") # load Universal Sentence Encoder
embed_samples = embed([sample_sentence,
                      "When you call the universal sentence encoder on a sentence, it turns it into numbers."])

print(embed_samples[0][:50])
```

    tf.Tensor(
    [-0.01157032  0.02485909  0.02878048 -0.01271501  0.03971539  0.0882776
      0.02680985  0.05589837 -0.0106873  -0.00597291  0.00639325 -0.0181952
      0.00030816  0.09105889  0.05874643 -0.03180627  0.01512473 -0.05162929
      0.00991365 -0.06865346 -0.04209305  0.02678981  0.03011008  0.00321067
     -0.0033797  -0.04787361  0.02266722 -0.00985925 -0.04063613 -0.0129209
     -0.04666385  0.056303   -0.03949255  0.00517688  0.02495828 -0.07014443
      0.02871508  0.04947681 -0.00633976 -0.08960193  0.02807116 -0.00808363
     -0.01360604  0.0599865  -0.10361787 -0.05195372  0.00232956 -0.02332528
     -0.03758106  0.03327731], shape=(50,), dtype=float32)

```python
# Each sentence has been encoded into a 512 dimension vector
embed_samples[0].shape
```

    TensorShape([512])

```python
# We can use this encoding layer in place of our text_vectorizer and embedding layer
sentence_encoder_layer = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4",
                                        input_shape=[], # shape of inputs coming to our model
                                        dtype=tf.string, # data type of inputs coming to the USE layer
                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)
                                        name="USE")
```

```python
# Create model using the Sequential API
model_6 = tf.keras.Sequential([
  sentence_encoder_layer, # take in sentences and then encode them into an embedding
  layers.Dense(64, activation="relu"),
  layers.Dense(1, activation="sigmoid")
], name="model_6_USE")

# Compile model
model_6.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

model_6.summary()
```

    Model: "model_6_USE"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #
    =================================================================
     USE (KerasLayer)            (None, 512)               256797824

     dense_5 (Dense)             (None, 64)                32832

     dense_6 (Dense)             (None, 1)                 65

    =================================================================
    Total params: 256,830,721
    Trainable params: 32,897
    Non-trainable params: 256,797,824
    _________________________________________________________________

Notice the number of paramters in the USE layer, these are the pretrained weights its learned on various text sources (Wikipedia, web news, web question-answer forums, etc, see the [Universal Sentence Encoder paper](https://www.aclweb.org/anthology/D18-2029.pdf) for more).

The trainable parameters are only in our output layers, in other words, we're keeping the USE weights frozen and using it as a feature-extractor. We could fine-tune these weights by setting `trainable=True` when creating the `hub.KerasLayer` instance.

```python
# Train a classifier on top of pretrained embeddings
model_6_history = model_6.fit(train_sentences,
                              train_labels,
                              epochs=15,
                              validation_data=(val_sentences, val_labels),
                              callbacks=[create_tensorboard_callback(SAVE_DIR,
                                                                     "tf_hub_sentence_encoder")])
```

    Saving TensorBoard log files to: model_logs/tf_hub_sentence_encoder/20220908-205828
    Epoch 1/15
    215/215 [==============================] - 5s 13ms/step - loss: 0.5008 - accuracy: 0.7892 - val_loss: 0.4478 - val_accuracy: 0.7966
    Epoch 2/15
    215/215 [==============================] - 2s 11ms/step - loss: 0.4144 - accuracy: 0.8133 - val_loss: 0.4369 - val_accuracy: 0.8058
    Epoch 3/15
    215/215 [==============================] - 2s 12ms/step - loss: 0.3998 - accuracy: 0.8212 - val_loss: 0.4329 - val_accuracy: 0.8110
    Epoch 4/15
    215/215 [==============================] - 2s 11ms/step - loss: 0.3925 - accuracy: 0.8266 - val_loss: 0.4288 - val_accuracy: 0.8110
    Epoch 5/15
    215/215 [==============================] - 3s 12ms/step - loss: 0.3860 - accuracy: 0.8276 - val_loss: 0.4309 - val_accuracy: 0.8123
    Epoch 6/15
    215/215 [==============================] - 3s 12ms/step - loss: 0.3789 - accuracy: 0.8329 - val_loss: 0.4257 - val_accuracy: 0.8123
    Epoch 7/15
    215/215 [==============================] - 3s 13ms/step - loss: 0.3727 - accuracy: 0.8335 - val_loss: 0.4249 - val_accuracy: 0.8136
    Epoch 8/15
    215/215 [==============================] - 3s 13ms/step - loss: 0.3663 - accuracy: 0.8380 - val_loss: 0.4249 - val_accuracy: 0.8241
    Epoch 9/15
    215/215 [==============================] - 3s 12ms/step - loss: 0.3596 - accuracy: 0.8408 - val_loss: 0.4292 - val_accuracy: 0.8150
    Epoch 10/15
    215/215 [==============================] - 3s 13ms/step - loss: 0.3532 - accuracy: 0.8450 - val_loss: 0.4291 - val_accuracy: 0.8163
    Epoch 11/15
    215/215 [==============================] - 3s 13ms/step - loss: 0.3468 - accuracy: 0.8467 - val_loss: 0.4273 - val_accuracy: 0.8176
    Epoch 12/15
    215/215 [==============================] - 4s 17ms/step - loss: 0.3403 - accuracy: 0.8536 - val_loss: 0.4290 - val_accuracy: 0.8202
    Epoch 13/15
    215/215 [==============================] - 4s 17ms/step - loss: 0.3330 - accuracy: 0.8559 - val_loss: 0.4298 - val_accuracy: 0.8176
    Epoch 14/15
    215/215 [==============================] - 3s 12ms/step - loss: 0.3267 - accuracy: 0.8591 - val_loss: 0.4331 - val_accuracy: 0.8202
    Epoch 15/15
    215/215 [==============================] - 2s 11ms/step - loss: 0.3179 - accuracy: 0.8615 - val_loss: 0.4315 - val_accuracy: 0.8202

```python
plot_loss_curves(model_6_history)
```

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_142_0.png)

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_142_1.png)

```python
# Make predictions with USE TF Hub model
model_6_pred_probs = model_6.predict(val_sentences)
model_6_pred_probs[:10]
```

    array([[0.11862903],
           [0.7678935 ],
           [0.998353  ],
           [0.2138451 ],
           [0.67368555],
           [0.79924685],
           [0.9932053 ],
           [0.99651635],
           [0.9643703 ],
           [0.03286057]], dtype=float32)

```python
# Convert prediction probabilities to labels
model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))
model_6_preds[:10]
```

    <tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>

```python
# Calculate model 6 performance metrics
model_6_results = calculate_results(val_labels, model_6_preds)
```

     Model accuracy is: 82.02
     Model precision is: 0.82
     Model recall is: 0.82
     Model f1-score is: 0.82

```python
# Compare TF Hub model to baseline
compare_baseline_to_new_results(baseline_results, model_6_results)
```

    Baseline accuracy: 79.27, New accuracy: 82.02, Difference: 2.76
    Baseline precision: 0.81, New precision: 0.82, Difference: 0.01
    Baseline recall: 0.79, New recall: 0.82, Difference: 0.03
    Baseline f1: 0.79, New f1: 0.82, Difference: 0.03

#### Same as model 6 with 10% of training data

One of the benefits of using transfer learning methods, such as, the pretrained embeddings within the USE is the ability to get great results on a small amount of data (the USE paper even mentions this in the abstract).

To put this to the test, we're going to make a small subset of the training data (10%), train a model and evaluate it.

```python
import numpy as np

train_sentences_90_percent, train_sentences_10_percent, train_labels_90_percent, train_labels_10_percent = train_test_split(np.array(train_sentences),
                                                                                                                            train_labels,
                                                                                                                            test_size=0.1,
                                                                                                                            random_state=42)

```

```python
# Check length of 10 percent datasets
print(f"Total training examples: {len(train_sentences)}")
print(f"Length of 10% training examples: {len(train_sentences_10_percent)}")
```

    Total training examples: 6851
    Length of 10% training examples: 686

Because we've selected a random subset of the training samples, the classes should be roughly balanced (as they are in the full training dataset).

```python
# Clone model_6 but reset weights
model_7 = tf.keras.models.clone_model(model_6)

# Compile model
model_7.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

# Get a summary (will be same as model_6)
model_7.summary()
```

    Model: "model_6_USE"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #
    =================================================================
     USE (KerasLayer)            (None, 512)               256797824

     dense_5 (Dense)             (None, 64)                32832

     dense_6 (Dense)             (None, 1)                 65

    =================================================================
    Total params: 256,830,721
    Trainable params: 32,897
    Non-trainable params: 256,797,824
    _________________________________________________________________

```python
# Fit the model to 10% of the training data
model_7_history = model_7.fit(x=train_sentences_10_percent,
                              y=train_labels_10_percent,
                              epochs=15,
                              validation_data=(val_sentences, val_labels),
                              callbacks=[create_tensorboard_callback(SAVE_DIR, "10_percent_tf_hub_sentence_encoder")])
```

    Saving TensorBoard log files to: model_logs/10_percent_tf_hub_sentence_encoder/20220908-205957
    Epoch 1/15
    22/22 [==============================] - 4s 44ms/step - loss: 0.6716 - accuracy: 0.6574 - val_loss: 0.6526 - val_accuracy: 0.6903
    Epoch 2/15
    22/22 [==============================] - 1s 26ms/step - loss: 0.5972 - accuracy: 0.8032 - val_loss: 0.5944 - val_accuracy: 0.7362
    Epoch 3/15
    22/22 [==============================] - 1s 26ms/step - loss: 0.5178 - accuracy: 0.8149 - val_loss: 0.5398 - val_accuracy: 0.7625
    Epoch 4/15
    22/22 [==============================] - 1s 25ms/step - loss: 0.4526 - accuracy: 0.8265 - val_loss: 0.5084 - val_accuracy: 0.7677
    Epoch 5/15
    22/22 [==============================] - 1s 26ms/step - loss: 0.4094 - accuracy: 0.8382 - val_loss: 0.4915 - val_accuracy: 0.7703
    Epoch 6/15
    22/22 [==============================] - 1s 26ms/step - loss: 0.3781 - accuracy: 0.8411 - val_loss: 0.4840 - val_accuracy: 0.7703
    Epoch 7/15
    22/22 [==============================] - 1s 25ms/step - loss: 0.3543 - accuracy: 0.8586 - val_loss: 0.4852 - val_accuracy: 0.7703
    Epoch 8/15
    22/22 [==============================] - 0s 21ms/step - loss: 0.3350 - accuracy: 0.8630 - val_loss: 0.4783 - val_accuracy: 0.7769
    Epoch 9/15
    22/22 [==============================] - 1s 25ms/step - loss: 0.3187 - accuracy: 0.8688 - val_loss: 0.4827 - val_accuracy: 0.7782
    Epoch 10/15
    22/22 [==============================] - 1s 25ms/step - loss: 0.3029 - accuracy: 0.8819 - val_loss: 0.4816 - val_accuracy: 0.7808
    Epoch 11/15
    22/22 [==============================] - 0s 21ms/step - loss: 0.2871 - accuracy: 0.8907 - val_loss: 0.4865 - val_accuracy: 0.7795
    Epoch 12/15
    22/22 [==============================] - 0s 21ms/step - loss: 0.2748 - accuracy: 0.9052 - val_loss: 0.4885 - val_accuracy: 0.7782
    Epoch 13/15
    22/22 [==============================] - 0s 21ms/step - loss: 0.2640 - accuracy: 0.9082 - val_loss: 0.4915 - val_accuracy: 0.7769
    Epoch 14/15
    22/22 [==============================] - 0s 21ms/step - loss: 0.2525 - accuracy: 0.9184 - val_loss: 0.4937 - val_accuracy: 0.7756
    Epoch 15/15
    22/22 [==============================] - 0s 22ms/step - loss: 0.2414 - accuracy: 0.9198 - val_loss: 0.4986 - val_accuracy: 0.7808

```python
plot_loss_curves(model_7_history)
```

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_154_0.png)

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_154_1.png)

```python
# Make predictions with the model trained on 10% of the data
model_7_pred_probs = model_7.predict(val_sentences)
model_7_pred_probs[:10]
```

    array([[0.1513885 ],
           [0.965229  ],
           [0.98655593],
           [0.1366746 ],
           [0.33930436],
           [0.95127946],
           [0.96952975],
           [0.90690035],
           [0.9502588 ],
           [0.04254567]], dtype=float32)

```python
# Convert prediction probabilities to labels
model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))
model_7_preds[:10]
```

    <tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>

```python
# Calculate model results
model_7_results = calculate_results(val_labels, model_7_preds)
```

     Model accuracy is: 78.08
     Model precision is: 0.78
     Model recall is: 0.78
     Model f1-score is: 0.78

## Comparing the performance of each of our models

To visualize our model's performances, let's create a pandas DataFrame we our results dictionaries and then plot it.

```python
# Combine model results into a DataFrame
all_model_results = pd.DataFrame({"baseline": baseline_results,
                                  "simple_dense": model_1_results,
                                  "lstm": model_2_results,
                                  "gru": model_3_results,
                                  "bidirectional": model_4_results,
                                  "conv1d": model_5_results,
                                  "tf_hub_sentence_encoder": model_6_results,
                                  "tf_hub_10_percent_data": model_7_results})
all_model_results = all_model_results.transpose()
all_model_results
```

  <div id="df-b3ef014f-608f-4c83-b85c-db6110c07b37">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>baseline</th>
      <td>79.265092</td>
      <td>0.811139</td>
      <td>0.792651</td>
      <td>0.786219</td>
    </tr>
    <tr>
      <th>simple_dense</th>
      <td>77.559055</td>
      <td>0.775418</td>
      <td>0.775591</td>
      <td>0.774788</td>
    </tr>
    <tr>
      <th>lstm</th>
      <td>76.115486</td>
      <td>0.760787</td>
      <td>0.761155</td>
      <td>0.760839</td>
    </tr>
    <tr>
      <th>gru</th>
      <td>74.803150</td>
      <td>0.748060</td>
      <td>0.748031</td>
      <td>0.746530</td>
    </tr>
    <tr>
      <th>bidirectional</th>
      <td>76.771654</td>
      <td>0.767545</td>
      <td>0.767717</td>
      <td>0.766793</td>
    </tr>
    <tr>
      <th>conv1d</th>
      <td>77.034121</td>
      <td>0.771589</td>
      <td>0.770341</td>
      <td>0.768449</td>
    </tr>
    <tr>
      <th>tf_hub_sentence_encoder</th>
      <td>82.020997</td>
      <td>0.822036</td>
      <td>0.820210</td>
      <td>0.818917</td>
    </tr>
    <tr>
      <th>tf_hub_10_percent_data</th>
      <td>78.083990</td>
      <td>0.783458</td>
      <td>0.780840</td>
      <td>0.778533</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-b3ef014f-608f-4c83-b85c-db6110c07b37')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
width="24px">
<path d="M0 0h24v24H0V0z" fill="none"/>
<path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
</svg>
</button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-b3ef014f-608f-4c83-b85c-db6110c07b37 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-b3ef014f-608f-4c83-b85c-db6110c07b37');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>

  </div>

```python
# Reduce the accuracy to same scale as other metrics
all_model_results["accuracy"] = all_model_results["accuracy"]/100
# Plot and compare all of the model results
all_model_results.plot(kind="bar", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0));
```

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_161_0.png)

Looks like our pretrained USE TensorFlow Hub models have the best performance, even the one with only 10% of the training data seems to outperform the other models. This goes to show the power of transfer learning.

```python
# Sort model results by f1-score
all_model_results.sort_values("f1", ascending=False)["f1"].plot(kind="bar", figsize=(10, 7));
```

![png](https://raw.githubusercontent.com/arminnorouzi/arminnorouzi.github.io/master/_posts/L04d_Natural_Language_Processing_with_TensorFlow_files/L04d_Natural_Language_Processing_with_TensorFlow_163_0.png)

Drilling down into a single metric we see our USE TensorFlow Hub models performing better than all of the other models. Interestingly, the baseline's F1-score isn't too far off the rest of the deeper models.

## Visualize in TensorBoard

```python
%load_ext tensorboard
%tensorboard --logdir model_logs
```

## Ensembling model

Many production systems use an **ensemble** (multiple different models combined) of models to make a prediction.

The idea behind model stacking is that if several uncorrelated models agree on a prediction, then the prediction must be more robust than a prediction made by a singular model.

Differetn method of ensemble learning:

1. **Averaging** - Take the output prediction probabilities of each model for each sample, combine them and then average them.
2. **Majority vote (mode)** - Make class predictions with each of your models on all samples, the predicted class is the one in majority. For example, if three different models predict `[1, 0, 1]` respectively, the majority class is `1`, therefore, that would be the predicted label.
3. **Model stacking** - Take the outputs of each of your chosen models and use them as inputs to another model.

```python
# Get mean pred probs for 3 models
baseline_pred_probs = np.max(model_0.predict_proba(val_sentences), axis=1) # get the prediction probabilities from baseline model
combined_pred_probs = baseline_pred_probs + tf.squeeze(model_2_pred_probs, axis=1) + tf.squeeze(model_6_pred_probs)
combined_preds = tf.round(combined_pred_probs/3) # average and round the prediction probabilities to get prediction classes
combined_preds[:20]
```

    <tf.Tensor: shape=(20,), dtype=float32, numpy=
    array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
           0., 1., 1.], dtype=float32)>

```python
# Calculate results from averaging the prediction probabilities
ensemble_results = calculate_results(val_labels, combined_preds)
```

     Model accuracy is: 77.69
     Model precision is: 0.78
     Model recall is: 0.78
     Model f1-score is: 0.78

```python
# Add our combined model's results to the results DataFrame
all_model_results.loc["ensemble_results"] = ensemble_results
# Convert the accuracy to the same scale as the rest of the results
all_model_results.loc["ensemble_results"]["accuracy"] = all_model_results.loc["ensemble_results"]["accuracy"]/100
```

```python
all_model_results
```

  <div id="df-f6cf04ab-fcfc-4904-893c-9551a8eedd8f">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>baseline</th>
      <td>0.792651</td>
      <td>0.811139</td>
      <td>0.792651</td>
      <td>0.786219</td>
    </tr>
    <tr>
      <th>simple_dense</th>
      <td>0.775591</td>
      <td>0.775418</td>
      <td>0.775591</td>
      <td>0.774788</td>
    </tr>
    <tr>
      <th>lstm</th>
      <td>0.761155</td>
      <td>0.760787</td>
      <td>0.761155</td>
      <td>0.760839</td>
    </tr>
    <tr>
      <th>gru</th>
      <td>0.748031</td>
      <td>0.748060</td>
      <td>0.748031</td>
      <td>0.746530</td>
    </tr>
    <tr>
      <th>bidirectional</th>
      <td>0.767717</td>
      <td>0.767545</td>
      <td>0.767717</td>
      <td>0.766793</td>
    </tr>
    <tr>
      <th>conv1d</th>
      <td>0.770341</td>
      <td>0.771589</td>
      <td>0.770341</td>
      <td>0.768449</td>
    </tr>
    <tr>
      <th>tf_hub_sentence_encoder</th>
      <td>0.820210</td>
      <td>0.822036</td>
      <td>0.820210</td>
      <td>0.818917</td>
    </tr>
    <tr>
      <th>tf_hub_10_percent_data</th>
      <td>0.780840</td>
      <td>0.783458</td>
      <td>0.780840</td>
      <td>0.778533</td>
    </tr>
    <tr>
      <th>ensemble_results</th>
      <td>0.776903</td>
      <td>0.777990</td>
      <td>0.776903</td>
      <td>0.777184</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-f6cf04ab-fcfc-4904-893c-9551a8eedd8f')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
width="24px">
<path d="M0 0h24v24H0V0z" fill="none"/>
<path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
</svg>
</button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-f6cf04ab-fcfc-4904-893c-9551a8eedd8f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-f6cf04ab-fcfc-4904-893c-9551a8eedd8f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>

  </div>

## Making predictions on the test dataset

```python
# Making predictions on the test dataset
test_sentences = test_df["text"].to_list()
test_samples = random.sample(test_sentences, 10)
for test_sample in test_samples:
  pred_prob = tf.squeeze(model_6.predict([test_sample])) # has to be list
  pred = tf.round(pred_prob)
  print(f"Pred: {int(pred)}, Prob: {pred_prob}")
  print(f"Text:\n{test_sample}\n")
  print("----\n")
```

    Pred: 0, Prob: 0.1934334933757782
    Text:
    02 03 04 05 AVALANCHE 1500 REAR AXLE ASSEMBLY 2055271 http://t.co/VxZhZsAlra http://t.co/HmXWRkbLS0

    ----

    Pred: 0, Prob: 0.08852536231279373
    Text:
    Why can't gay men donate blood? http://t.co/v2Etl8P9eQ http://t.co/NLnyzeljbw

    ----

    Pred: 0, Prob: 0.052361153066158295
    Text:
    Craving slurpees ;-;

    ----

    Pred: 0, Prob: 0.13187173008918762
    Text:
    Finnish Nuclear Plant to Move Ahead After Financing Secured http://t.co/S9Jhcf3lD7 @JukkaOksaharju @ollirehn @juhasipila

    ----

    Pred: 1, Prob: 0.9913582801818848
    Text:
    Miners strike; Botha Apartheid SA Tutu Nobel prize; IRA bomb Brighton  Famine in Ethiopia. '84 feels like yesterday http://t.co/UZKssvj9CW

    ----

    Pred: 1, Prob: 0.9993112087249756
    Text:
    At this hour 70 yrs ago one of the greatest acts of mass murder in world history occurred when #Hiroshima was bombed. http://t.co/2QJl5MfKzv

    ----

    Pred: 1, Prob: 0.9734219908714294
    Text:
    Evacuation order lifted for town of Roosevelt - Washington Times http://t.co/Kue48Nmjxh

    ----

    Pred: 0, Prob: 0.008846422657370567
    Text:
    did harry survive the show

    ----

    Pred: 1, Prob: 0.9872232675552368
    Text:
    Traffic Collision - Ambulance Enroute: Florin Rd at Franklin Blvd South Sac http://t.co/dYEl9nMQ0A

    ----

    Pred: 0, Prob: 0.23675793409347534
    Text:
    THIS SOUNDS LIKE A SONG YOU WOULD HEAR IN A MOVIE WHERE THEY ARE WALKING AWAY FROM BURNING BUILDINGS AND CARS AND SHIT

    ----

### Predicting on Tweets from the wild

```python
# Turn Tweet into string
daniels_tweet = "Life like an ensemble: take the best choices from others and make your own"
```

```python
def predict_on_sentence(model, sentence):
  """
  Uses model to make a prediction on sentence.

  Returns the sentence, the predicted label and the prediction probability.
  """
  pred_prob = model.predict([sentence])
  pred_label = tf.squeeze(tf.round(pred_prob)).numpy()
  print(f"Pred: {pred_label}", "(real disaster)" if pred_label > 0 else "(not real disaster)", f"Prob: {pred_prob[0][0]}")
  print(f"Text:\n{sentence}")
```

```python
# Make a prediction on Tweet from the wild
predict_on_sentence(model=model_6, # use the USE model
                    sentence=daniels_tweet)
```

    Pred: 0.0 (not real disaster) Prob: 0.08231339603662491
    Text:
    Life like an ensemble: take the best choices from others and make your own

```python
# Source - https://twitter.com/BeirutCityGuide/status/1290696551376007168
beirut_tweet_1 = "Reports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon"

# Source - https://twitter.com/BeirutCityGuide/status/1290773498743476224
beirut_tweet_2 = "#Beirut declared a “devastated city”, two-week state of emergency officially declared. #Lebanon"
```

```python
# Predict on diaster Tweet 1
predict_on_sentence(model=model_6,
                    sentence=beirut_tweet_1)
```

    Pred: 1.0 (real disaster) Prob: 0.9933708906173706
    Text:
    Reports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon

```python
# Predict on diaster Tweet 2
predict_on_sentence(model=model_6,
                    sentence=beirut_tweet_2)
```

    Pred: 1.0 (real disaster) Prob: 0.9962399005889893
    Text:
    #Beirut declared a “devastated city”, two-week state of emergency officially declared. #Lebanon

## References:

- [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)
- [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)
- [TensorFlow Transfer Learning Guide](https://www.tensorflow.org/tutorials/images/transfer_learning)
- [Transfer Learning with TensorFlow Hub tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)
- [fine-tuning a TensorFlow Hub model tutorial](https://www.tensorflow.org/hub/tf2_saved_model#fine-tuning)
- [experiment tracking with Weights & Biases](https://www.wandb.com/experiment-tracking)

```python

```
